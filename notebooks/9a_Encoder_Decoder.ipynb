{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orlandxrf/curso-dl/blob/main/notebooks/9a_Encoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgwZXA-ujlLa"
      },
      "source": [
        "# Encoder - Decoder\n",
        "Traducción automática con una red Encoder-Decoder<br>\n",
        "<br>\n",
        "Machine Translation del Alemán al Inglés"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J1boXxOoVr8"
      },
      "source": [
        "## Bibilotecas a usar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iyISNfLYjXTe"
      },
      "outputs": [],
      "source": [
        "# bibliotecas a usar\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cf7yd8u3oOTl"
      },
      "outputs": [],
      "source": [
        "# utilizar el conjunto de datos Multi30K\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "# más información sobre el dataset Multi30K: https://pytorch.org/text/stable/datasets.html#multi30k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WaQ-yNtxpGrs"
      },
      "outputs": [],
      "source": [
        "# utilizar spacy para tokenizar los lenguajes\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq7AtNcwpUAE"
      },
      "source": [
        "## Conjuntos de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbJNap-BpW3c",
        "outputId": "83c42cfe-99ca-4fc8-bee1-e1215b0a51a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.0 MB 778 kB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "\u001b[K     |████████████████████████████████| 14.9 MB 891 kB/s \n",
            "\u001b[?25h  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en --quiet\n",
        "!python -m spacy download de --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XmxP5twbperk"
      },
      "outputs": [],
      "source": [
        "# cargar los modelos (modelo pequeño) de los lenguajes\n",
        "spacy_english = spacy.load(\"en\")\n",
        "spacy_german = spacy.load(\"de\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gxxb7qL8p7qs"
      },
      "outputs": [],
      "source": [
        "# funciones para tokenizar\n",
        "def tokenize_german(text):\n",
        "  return [token.text for token in spacy_german.tokenizer(text)]\n",
        "\n",
        "def tokenize_english(text):\n",
        "  return [token.text for token in spacy_english.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "usZnJlsdqHSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9085a7b-52cf-45e7-e0cd-8a913bd8208a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading training.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.21M/1.21M [00:04<00:00, 268kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading validation.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46.3k/46.3k [00:00<00:00, 91.6kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66.2k/66.2k [00:00<00:00, 88.3kB/s]\n"
          ]
        }
      ],
      "source": [
        "# crear objetos Field para cada uno de los idiomas\n",
        "german = Field(\n",
        "    tokenize=tokenize_german,\n",
        "    lower=True,\n",
        "    init_token=\"<sos>\",\n",
        "    eos_token=\"<eos>\"\n",
        ")\n",
        "\n",
        "english = Field(\n",
        "    tokenize=tokenize_english,\n",
        "    lower=True,\n",
        "    init_token=\"<sos>\",\n",
        "    eos_token=\"<eos>\"\n",
        ")\n",
        "\n",
        "# 29,000  train\n",
        "#  1,014  valid\n",
        "#  1,000  test\n",
        "\n",
        "# obtener los conjutos de datos: entrenamiento, validación y prueba de ambos idiomas\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts=(\".de\", \".en\"), fields=(german, english))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Kq-er6r3YN",
        "outputId": "e01ccdba-95b8-411b-8848-3b8201fa5e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 5.4M\n",
            "drwxr-xr-x 2 root root 4.0K Mar 24 15:42 .\n",
            "drwxr-xr-x 3 root root 4.0K Mar 24 15:42 ..\n",
            "-rw-r--r-- 1 root root  65K Mar 24 15:42 mmt_task1_test2016.tar.gz\n",
            "-rw-rw-r-- 1 1000 1000  69K Oct 17  2016 test2016.de\n",
            "-rw-rw-r-- 1 1000 1000  61K Oct 17  2016 test2016.en\n",
            "-rw-rw-r-- 1 1000 1000  71K Feb 11  2017 test2016.fr\n",
            "-rw-rw-r-- 1 1000 1000 2.1M Feb  2  2016 train.de\n",
            "-rw-rw-r-- 1 1000 1000 1.8M Feb  2  2016 train.en\n",
            "-rw-r--r-- 1 root root 1.2M Mar 24 15:42 training.tar.gz\n",
            "-rw-rw-r-- 1 1000 1000  75K Feb  2  2016 val.de\n",
            "-rw-rw-r-- 1 1000 1000  62K Feb  2  2016 val.en\n",
            "-rw-r--r-- 1 root root  46K Mar 24 15:42 validation.tar.gz\n"
          ]
        }
      ],
      "source": [
        "!ls -lha .data/multi30k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WOp1LuX2q0S0"
      },
      "outputs": [],
      "source": [
        "# construir los vocabularios\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=3)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP82Fjt4rF6d",
        "outputId": "2806cd2d-5262-48bf-ac0d-3ffe791e8829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens únicos en el vocabulario del idioma italiano: 5,376\n",
            "Tokens únicos en el vocabulario del idioma inglés: 4,556\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokens únicos en el vocabulario del idioma italiano: {len(german.vocab):,}\")\n",
        "print(f\"Tokens únicos en el vocabulario del idioma inglés: {len(english.vocab):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RIko0b4tredt"
      },
      "outputs": [],
      "source": [
        "# verificar dispositivo a usar (cpu o cuda)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iVQAymGXu3UW"
      },
      "outputs": [],
      "source": [
        "# definir el tamaño del batch\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BtEmqGgyu4b-"
      },
      "outputs": [],
      "source": [
        "# crear los iteradores\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
        "                                                                      batch_size = BATCH_SIZE, \n",
        "                                                                      sort_within_batch=True,\n",
        "                                                                      sort_key=lambda x: len(x.src),\n",
        "                                                                      device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfDOCTUgvRC8",
        "outputId": "f67c5919-c41e-4948-bbc5-91ec066bce30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29,000 muestras en el conjunto de entrenamiento\n",
            "1,014 muestras en el conjunto de validación\n",
            "1,000 muestras en el conjunto de prueba\n",
            "\n",
            "907 batches de tamaño 32 en el conjunto de entrenamiento\n",
            "32 batches de tamaño 32 en el conjunto de validación\n",
            "32 batches de tamaño 32 en el conjunto de prueba\n"
          ]
        }
      ],
      "source": [
        "print (f\"{len(train_iterator.dataset):,} muestras en el conjunto de entrenamiento\")\n",
        "print (f\"{len(valid_iterator.dataset):,} muestras en el conjunto de validación\")\n",
        "print (f\"{len(test_iterator.dataset):,} muestras en el conjunto de prueba\\n\")\n",
        "\n",
        "print (f\"{len(train_iterator):,} batches de tamaño {BATCH_SIZE} en el conjunto de entrenamiento\")\n",
        "print (f\"{len(valid_iterator):,} batches de tamaño {BATCH_SIZE} en el conjunto de validación\")\n",
        "print (f\"{len(test_iterator):,} batches de tamaño {BATCH_SIZE} en el conjunto de prueba\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVlh7ZrnwCQt",
        "outputId": "ce3ccd8b-f4ec-4e85-f268-d6bfab0cf021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alemán: zwei junge weiße männer sind im freien in der nähe vieler büsche . Length: 13\n",
            "Inglés: two young , white males are outside near many bushes . Length: 11\n",
            "\n",
            "Alemán: mehrere männer mit schutzhelmen bedienen ein antriebsradsystem . Length: 8\n",
            "Inglés: several men in hard hats are operating a giant pulley system . Length: 12\n",
            "\n",
            "Alemán: ein kleines mädchen klettert in ein spielhaus aus holz . Length: 10\n",
            "Inglés: a little girl climbing into a wooden playhouse . Length: 9\n",
            "\n",
            "Alemán: ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster . Length: 15\n",
            "Inglés: a man in a blue shirt is standing on a ladder cleaning a window . Length: 15\n",
            "\n",
            "Alemán: zwei männer stehen am herd und bereiten essen zu . Length: 10\n",
            "Inglés: two men are at the stove preparing food . Length: 9\n",
            "\n",
            "Alemán: ein mann in grün hält eine gitarre , während der andere mann sein hemd ansieht . Length: 16\n",
            "Inglés: a man in green holds a guitar while the other man observes his shirt . Length: 15\n",
            "\n",
            "Alemán: ein mann lächelt einen ausgestopften löwen an . Length: 8\n",
            "Inglés: a man is smiling at a stuffed lion Length: 8\n",
            "\n",
            "Alemán: ein schickes mädchen spricht mit dem handy während sie langsam die straße entlangschwebt . Length: 14\n",
            "Inglés: a trendy girl talking on her cellphone while gliding slowly down the street . Length: 14\n",
            "\n",
            "Alemán: eine frau mit einer großen geldbörse geht an einem tor vorbei . Length: 12\n",
            "Inglés: a woman with a large purse is walking by a gate . Length: 12\n",
            "\n",
            "Alemán: jungen tanzen mitten in der nacht auf pfosten . Length: 9\n",
            "Inglés: boys dancing on poles in the middle of the night . Length: 11\n",
            "\n",
            "Oración con longitud máxima y mínima en alemán:\t44\t1\n",
            "Oración con longitud máxima y mínima en inglés:\t41\t4\n"
          ]
        }
      ],
      "source": [
        "sent_len_ger, sent_len_eng = [], []\n",
        "for i, data in enumerate(train_data): \n",
        "    sent_len_ger.append(len(data.src))\n",
        "    sent_len_eng.append(len(data.trg))\n",
        "    if i < 10 :\n",
        "        print(f\"Alemán: {' '.join(data.src)} Length: {len(data.src)}\")\n",
        "        print(f\"Inglés: {' '.join(data.trg)} Length: {len(data.trg)}\\n\")\n",
        "\n",
        "print(f\"Oración con longitud máxima y mínima en alemán:\\t{max(sent_len_ger)}\\t{min(sent_len_ger)}\")\n",
        "print(f\"Oración con longitud máxima y mínima en inglés:\\t{max(sent_len_eng)}\\t{min(sent_len_eng)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5x81Cewwe5l"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ySNE_xj5A6"
      },
      "source": [
        "## Definir el modelo Encoder-Decoder\n",
        "En principio se define el encoder, que es quien recibe las entradas. En este caso las entradas corresponden al idioma Alemán."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-SAcCnV2hjn"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ra4OtYtjjzWq"
      },
      "outputs": [],
      "source": [
        "# definir el encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Size of the one hot vectors that will be the input to the encoder\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Output size of the word embedding NN\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Number of layers in the lstm\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Regularization parameter\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.tag = True\n",
        "\n",
        "        # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        \n",
        "        # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "        self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "    # Shape of x (26, 32) [Sequence_length, batch_size]\n",
        "    def forward(self, x):\n",
        "        # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        \n",
        "        # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n",
        "        # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n",
        "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "        return hidden_state, cell_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI-yi_yz3LTG",
        "outputId": "e1be3966-df85-448f-8a39-25301efe2027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(5376, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# instanciar el encoder\n",
        "input_size_encoder = len(german.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = float(0.5)\n",
        "\n",
        "encoder = Encoder(\n",
        "    input_size_encoder, \n",
        "    encoder_embedding_size,\n",
        "    hidden_size, \n",
        "    num_layers, \n",
        "    encoder_dropout\n",
        ").to(device)\n",
        "\n",
        "print(encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byJreosO2kDA"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hiVOtttU2ljA"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Size of the one hot vectors that will be the input to the encoder\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Output size of the word embedding NN\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Number of layers in the lstm\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Regularization parameter\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.tag = True\n",
        "\n",
        "        # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
        "\n",
        "        # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "        self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "        # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
        "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    # Shape of x (32) [batch_size]\n",
        "    def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "        # Shape of x (1, 32) [1, batch_size]\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
        "        # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "        outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "        # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        return predictions, hidden_state, cell_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0XJQ5B32_Iq",
        "outputId": "a825bb2f-1d2a-4134-fbbc-8102efda9174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(4556, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=4556, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# instanciar el decoder\n",
        "input_size_decoder = len(english.vocab) \n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = float(0.5)\n",
        "output_size = len(english.vocab)\n",
        "\n",
        "decoder = Decoder(\n",
        "    input_size_decoder, \n",
        "    decoder_embedding_size,\n",
        "    hidden_size, num_layers, \n",
        "    decoder_dropout, output_size\n",
        ").to(device)\n",
        "\n",
        "print(decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGIJJlwd3z55"
      },
      "source": [
        "## Definir el modelo Seq2Seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WTAmAguC35_p"
      },
      "outputs": [],
      "source": [
        "# definir el modelo seq2seq que contiene al encoder y decoder\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, Encoder, Decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.Encoder = Encoder\n",
        "        self.Decoder = Decoder\n",
        "\n",
        "    def forward(self, source, target, tfr=0.5):\n",
        "        # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n",
        "        batch_size = source.shape[1]\n",
        "\n",
        "        # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(english.vocab)\n",
        "        \n",
        "        # Shape --> outputs (14, 32, 5766) \n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
        "        hidden_state_encoder, cell_state_encoder = self.Encoder(source)\n",
        "\n",
        "        # Shape of x (32 elements)\n",
        "        x = target[0] # Trigger token <SOS>\n",
        "\n",
        "        for i in range(1, target_len):\n",
        "            # Shape --> output (32, 5766) \n",
        "            output, hidden_state_decoder, cell_state_decoder = self.Decoder(x, hidden_state_encoder, cell_state_encoder)\n",
        "            outputs[i] = output\n",
        "            best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "            x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "        # Shape --> outputs (14, 32, 5766) \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luF6fSG64PNg",
        "outputId": "99f291a5-6e12-4104-e881-be2100eecad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (Encoder): Encoder(\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (embedding): Embedding(5376, 300)\n",
            "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  )\n",
            "  (Decoder): Decoder(\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (embedding): Embedding(4556, 300)\n",
            "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "    (fc): Linear(in_features=1024, out_features=4556, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# instanciar el modelo principal (seq2seq)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YukK1vv846Kx"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku6rk_hG6kpx"
      },
      "source": [
        "### Hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "41IiVXYD6we4"
      },
      "outputs": [],
      "source": [
        "# Hyperparámetros\n",
        "learning_rate = 0.001\n",
        "step = 0\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"] # obtener id del relleno\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx) # omitir calcular elementos con relleno en el backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "25Ett4aF76MK"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    spacy_ger = spacy.load(\"de\")\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WYhNHMum7UbB"
      },
      "outputs": [],
      "source": [
        "def bleu(data, model, german, english, device):\n",
        "    \"\"\"Método para medir el rendimiento del modelo\"\"\"\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "y8ExkUuK8tly"
      },
      "outputs": [],
      "source": [
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    print('saving')\n",
        "    print()\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, '/content/checkpoint-NMT.ckpt')\n",
        "    torch.save(model.state_dict(),'/content/checkpoint-NMT-SD.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_Dn3wtxKCohT"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    \"\"\"Obtener el tiempo en minutos y segundos\"\"\"\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "ZyTd4Y6u476I",
        "outputId": "27f1d9f3-117d-4489-c158-7ca2fe0c48e3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-18221d1844b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Calculate the gradients for weights & biases using back-propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Clip the gradient value is it exceeds > 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "num_epochs = 100\n",
        "# best_loss = 999999\n",
        "# best_epoch = -1\n",
        "sentence1 = \"ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster\"\n",
        "epoch_loss_list = []\n",
        "ts1 = []\n",
        "\n",
        "start_total_time = time.time() # inicializar \n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    # calcular el tiempo que tarda cada epoca\n",
        "    start_time = time.time() # inicializar\n",
        "\n",
        "    model.train(True)\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        input = batch.src.to(device)\n",
        "        target = batch.trg.to(device)\n",
        "\n",
        "        # Pass the input and target for model's forward method\n",
        "        output = model(input, target)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        # Clear the accumulating gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate the loss value for every epoch\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Calculate the gradients for weights & biases using back-propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient value is it exceeds > 1\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Update the weights values using the gradients we calculated using bp \n",
        "        optimizer.step()\n",
        "        step += 1\n",
        "        epoch_loss += loss.item()\n",
        "        # writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "\n",
        "    # if epoch_loss < best_loss:\n",
        "    #   best_loss = epoch_loss\n",
        "    #   best_epoch = epoch\n",
        "    #   checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "    #   if ((epoch - best_epoch) >= 10):\n",
        "    #     print(\"no improvement in 10 epochs, break\")\n",
        "    #     break\n",
        "\n",
        "    if epoch_loss < best_valid_loss: # guardar el mejor modelo\n",
        "        best_valid_loss = epoch_loss\n",
        "        checkpoint_and_save(model, best_valid_loss, epoch, optimizer, epoch_loss) \n",
        "      \n",
        "    tmp_loss = epoch_loss / len(train_iterator)\n",
        "    epoch_loss_list.append(tmp_loss)\n",
        "    \n",
        "    end_time = time.time() # obtener el tiempo final\n",
        "    # obtener los minutos y segundos del tiempo de ejecución\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # ----------------------------------------------------------------------------------\n",
        "    # evaluación\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        translated_sentence1 = translate_sentence(model, sentence1, german, english, device, max_length=50)\n",
        "        print(f\"Translated example sentence 1:\\n{' '.join(translated_sentence1)}\")\n",
        "        ts1.append(translated_sentence1)\n",
        "    # ----------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"Epoch - {epoch+1} / {num_epochs} | Epoch Time:\\t{epoch_mins}m {epoch_secs}s | Epoch_Loss:\\t{tmp_loss:.3f}\")\n",
        "    print()\n",
        "  \n",
        "print(epoch_loss / len(train_iterator))\n",
        "\n",
        "score = bleu(test_data[1:100], model, german, english, device)\n",
        "print(f\"Bleu score {score*100:.2f}\")\n",
        "\n",
        "end_total_time = time.time() # obtener el tiempo final\n",
        "epoch_mins, epoch_secs = epoch_time(start_total_time, end_total_time)\n",
        "print (f\"\\nTotal time for training model:\\t{epoch_mins}m {epoch_secs}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1988z5HbZG5b"
      },
      "source": [
        "<pre>\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue a a a a game of a group of a swing thin school gyro touch heels short awaits demonstrates wheelchairs wires expanse oxford loading oxford removing tethered vegetation sunrise multicolor cardigan janitor tomato cubicle projected secured snap surgery amid fancy bites apart attentively down challenge religious things crate\n",
        "Epoch - 1 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.926\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in a a a game of red striped hardwood discuss yelling tulips pause waited ally happening slam slide passersby sweatshirt violinist pack crotch rack backhoe propane attaches asking texas dunk turbans aqua tye tye ripped laundromat law law bunny landed procession sheepdog solitary 30 bagpipe law law straining\n",
        "Epoch - 2 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.891\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue is on a a down some a baseball white five used used zone purse twirl mustachioed she can contraption mad mad weathered excitement heavyset gyro bookstore janitor law classic law scissors bookstore cars wakeboard janitor law law enforcement law law pajamas miami junk metallic reach onlooker crab three\n",
        "Epoch - 3 / 30 | Epoch Time:\t3m 30s | Epoch_Loss:\t1.859\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a window a a a small a small a small a girls smiles force french constructed honor related participants aerial participants california stay healthcare fog fog fog graying judges major streams how diverse box shop trash rollerblader rollerblader elbow dozen twirls rollerblader solitary wicker forested shelter stall mountaintop scenic act\n",
        "Epoch - 4 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.829\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue on a a a small <unk> a laugh group of a good length climb m rod dragon parts mustard return shaded handbag electrician cycles briefcase topped damaged rainstorm solitary claus beige hills preparation teal bearer mustard years shaded outfits spectacular shaded projected pitched metropolitan shaded create fills pokes\n",
        "Epoch - 5 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.797\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue a a group of worker sitting road abandoned moving vietnamese personnel breaking congregating dumpster college pub seemingly paneled monkey honor gyro features studies ohio strikes equipped butchering competes on a a good picture of straw stone figure points dough with resting himself with tasty bones features sprinkled excitedly\n",
        "Epoch - 6 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.772\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue a a a football of young men carve cord technique memorial bears squat features bears lounge flames waring bears paneled doo motorboat tye sprays air track . <eos>\n",
        "Epoch - 7 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.750\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue . <eos>\n",
        "Epoch - 8 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.732\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in a a group of smoking a red boat <eos>\n",
        "Epoch - 9 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.705\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in the <eos>\n",
        "Epoch - 10 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.694\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in a a large crowd this ready picture picture of a cement baggage road children pain fists pain veteran pain pain veteran unique toe drainage used cry chart where surfers elevator elevator primitive primitive honor toss features unfinished cabin socks opens salad walker stride mark butt crying barber\n",
        "Epoch - 11 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.656\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue on the inside building . <eos>\n",
        "Epoch - 12 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.639\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in a group of enjoys dirty displays design tool tool . <eos>\n",
        "Epoch - 13 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.629\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue standing on on a the frisbee . <eos>\n",
        "Epoch - 14 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.585\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue on the counter belts portable ticket jumpsuits sprinkled jogger beagle toyota karaoke bowler metropolitan corral rollerblader rollerblader dozens sprinkles forrest aerial left . <eos>\n",
        "Epoch - 15 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.575\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue standing on on a a a small crowd <eos>\n",
        "Epoch - 16 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.560\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue blue standing in on horses three animal camouflage horns corral scantily heavyset frozen defends jack find bulls angry fit short business business wear decorations laundromat safe suspended up belts obama tasty flowery hikes pony pony latino jet schoolgirl law operated ways stacking views an starring fork instructor rainstorm\n",
        "Epoch - 17 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.539\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in the water with aprons panda observing microscopes shooting a lap shot of gets bread help cup pot board . <eos>\n",
        "Epoch - 18 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.535\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a window . <eos>\n",
        "Epoch - 19 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.522\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue standing on a an three leg picture ready into panda depicting pigtails under clutching carrying still food a girl who yellow snaps mcdonalds waterskier law law waterskier waterskier law law warehouse maneuver firetrucks painters painters texas healthcare better better dozen crouches priest lets sooners personnel chart begging paying\n",
        "Epoch - 20 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.490\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue while one congregate gliding signing reflective vests butchering names names tutu barriers mustard visible follow grabbing yelling island zebra tattered attaches bananas field . tan range art sits karate pose pose cross painter painter dugout law braces toss ragged ref dozens texts tiles elevator toss troops crayon jumpsuits\n",
        "Epoch - 21 / 30 | Epoch Time:\t3m 26s | Epoch_Loss:\t1.495\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue standing on <eos>\n",
        "Epoch - 22 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.479\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue while on displays some blown draped place in some long feet foreign studio lines leaves . <eos>\n",
        "Epoch - 23 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.448\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue standing on <eos>\n",
        "Epoch - 24 / 30 | Epoch Time:\t3m 26s | Epoch_Loss:\t1.448\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue blue on the grass pool <eos>\n",
        "Epoch - 25 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.437\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue standing on the an three caught japanese suspended 6 descent lets using laptops . <eos>\n",
        "Epoch - 26 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.413\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue while on a mountain glass room bull outdoors . <eos>\n",
        "Epoch - 27 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.398\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue is standing on <eos>\n",
        "Epoch - 28 / 30 | Epoch Time:\t3m 27s | Epoch_Loss:\t1.401\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue on the bar military lights cigar leaving unique bamboo <unk> fountains garden . <eos>\n",
        "Epoch - 29 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.393\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue standing on <eos>\n",
        "Epoch - 30 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.371\n",
        "\n",
        "1.3713945452834437\n",
        "Bleu score 1.87\n",
        "\n",
        "Total time for training model:\t107m 43s\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMeXVn-fo7ym"
      },
      "source": [
        "<pre>\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a a a man . <eos>\n",
        "Epoch - 1 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t4.719\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man in a a a a . <eos>\n",
        "Epoch - 2 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t3.985\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man . <eos>\n",
        "Epoch - 3 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t3.576\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man in a a . <eos>\n",
        "Epoch - 4 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t3.333\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man in a a a a a a a a a a . <eos>\n",
        "Epoch - 5 / 20 | Epoch Time:\t3m 30s | Epoch_Loss:\t3.132\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man in a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
        "Epoch - 6 / 20 | Epoch Time:\t3m 31s | Epoch_Loss:\t2.965\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue man . <eos>\n",
        "Epoch - 7 / 20 | Epoch Time:\t3m 30s | Epoch_Loss:\t2.848\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man in a a a a a a a a a a a young a small . <eos>\n",
        "Epoch - 8 / 20 | Epoch Time:\t3m 31s | Epoch_Loss:\t2.721\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man in a a a a a a a a a a a a a a a a a a a a a a a a a a a a a child <eos>\n",
        "Epoch - 9 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.605\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man in a a a a . <eos>\n",
        "Epoch - 10 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.525\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue man <eos>\n",
        "Epoch - 11 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.432\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a man in a a a <unk> a <unk> a <unk> a . <eos>\n",
        "Epoch - 12 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t2.354\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue a a a blue <eos>\n",
        "Epoch - 13 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.286\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in a a a a a a a a a a a a a a large crowd of a a a red <unk> is hanging in the side the side of a a large group of a red of water <eos>\n",
        "Epoch - 14 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.230\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in a a a a a a a a a a soccer of a a <unk> a a a <unk> white race is taking a a a a race a <unk> a <unk> piece of a jumper burger sitting in the real troops real reclines sits in side\n",
        "Epoch - 15 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t2.188\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue a man is <eos>\n",
        "Epoch - 16 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t2.139\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue in a a a small <unk> a <unk> a giant crowd of bright white aim handicapped short stripes m softball pose . <eos>\n",
        "Epoch - 17 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.075\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue blue a a <unk> <eos>\n",
        "Epoch - 18 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t2.033\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue a blue a race of a football race of a football . <eos>\n",
        "Epoch - 19 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.991\n",
        "\n",
        "saving\n",
        "\n",
        "Translated example sentence 1:\n",
        "a blue a blue <unk> looking like a a small <unk> stand in the <unk> striped top bathing vibrant soda pillar boys painters navigating rack last moose grasping lively save shadowed pro bush shaded winter skirt wicker slightly muscular statute special aim posters thumb begins chubby crystal plane dunes .\n",
        "Epoch - 20 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.962\n",
        "\n",
        "1.961763057056835\n",
        "Bleu score 2.50\n",
        "\n",
        "Total time for training model:\t72m 28s\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drl-0QIhGtPt"
      },
      "outputs": [],
      "source": [
        "# graficar las perdidas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print (f\"Epochs: {len(epoch_loss_list)}\")\n",
        "\n",
        "# Graficar accuracy y loss\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "plt.plot(epoch_loss_list, label='Training loss', marker='o', color='orange')\n",
        "plt.title('Losses', fontsize=15)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.savefig('/content/loss.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ9Bm2eNq95_"
      },
      "source": [
        "### Descargar el mejor modelo entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u78wkSGbrENX"
      },
      "outputs": [],
      "source": [
        "# biblioteca de google colab para descargar o subir archivos\n",
        "from google.colab import files\n",
        "\n",
        "# subir archivos con un widget\n",
        "# files.upload()\n",
        "\n",
        "checkpoint = '/content/checkpoint-NMT-SD.ckpt'\n",
        "\n",
        "# descargar el mejor modelo\n",
        "files.download(checkpoint)\n",
        "\n",
        "# descargar gráfica de perdidas\n",
        "files.download('/content/loss.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "9a_Encoder_Decoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMb0n+iAbwuezTUQsgtQt8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}