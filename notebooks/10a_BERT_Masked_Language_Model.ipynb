{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10a_BERT_Masked_Language_Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMgotWjn79DkZAHhyNsgc25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orlandxrf/curso-dl/blob/main/notebooks/10a_BERT_Masked_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masked Language Model\n",
        "\n",
        "Mas sobre [Fill-Mask](https://huggingface.co/tasks/fill-mask)"
      ],
      "metadata": {
        "id": "ozxsUtUMJGRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB4NrvMrJQHA",
        "outputId": "782edf39-74ad-488b-cf24-ee1c992da3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método 1"
      ],
      "metadata": {
        "id": "-i8KcGqIqG-z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dodevSkyJAmT"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Cambiar el modelo pre-entrenado para el idioma español.\n",
        "\n",
        "[Modelos de Hugging Face](https://huggingface.co/models)\n",
        "\n",
        "Filtrar modelos por `Fill-Mask` y `Language`"
      ],
      "metadata": {
        "id": "8uSpWf7kKuQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"bert-base-uncased\""
      ],
      "metadata": {
        "id": "xFFq3g-FJuWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MLM pipeline\n",
        "mlm = pipeline('fill-mask', model=model_checkpoint)\n",
        "\n",
        "# Get mask token\n",
        "mask = mlm.tokenizer.mask_token\n"
      ],
      "metadata": {
        "id": "Ez4_uyNqjj66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183e03da-6bb1-480a-8bd1-e066db85aac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frase = f'what is {mask} name?'\n",
        "\n",
        "result = mlm(frase)\n",
        "\n",
        "for x in result:\n",
        "    print (x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cDWl80Oktor",
        "outputId": "35bedc65-6b9b-419d-f2ed-fcbded622e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.5362831950187683, 'token': 2115, 'token_str': 'your', 'sequence': 'what is your name?'}\n",
            "{'score': 0.2603796124458313, 'token': 2014, 'token_str': 'her', 'sequence': 'what is her name?'}\n",
            "{'score': 0.14665333926677704, 'token': 2010, 'token_str': 'his', 'sequence': 'what is his name?'}\n",
            "{'score': 0.03641762584447861, 'token': 2026, 'token_str': 'my', 'sequence': 'what is my name?'}\n",
            "{'score': 0.004835779312998056, 'token': 2049, 'token_str': 'its', 'sequence': 'what is its name?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método 2"
      ],
      "metadata": {
        "id": "YtW1vQB0nGbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "ys_UlFeYnIe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb6b541-7719-4f9c-fc6d-59b8a27bdcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizar la cantidad de parámetros de este checkpoint (modelo)\n",
        "parameters_number = model.num_parameters() / 1_000_000\n",
        "\n",
        "print(f\">>> {model_checkpoint}: {round(parameters_number)}M\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H008IsqnNsk",
        "outputId": "8a42b3ff-51a1-4b7f-8398-e417b9bf5f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> bert-base-uncased: 110M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "sAUpTCmBn3Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Get mask token\n",
        "mask = mlm.tokenizer.mask_token\n",
        "\n",
        "frase = f'data science {mask} will not pass the course'\n",
        "\n",
        "inputs = tokenizer(frase, return_tensors=\"pt\")\n",
        "\n",
        "print (f\"inputs:\\t{inputs}\\n\")\n",
        "print (f\"tokenizer.decode([token]):\\t{tokenizer.decode(inputs.input_ids[0])}\\n\")\n",
        "\n",
        "print (f\"tokenizer.mask_token:\\t{tokenizer.mask_token}\")\n",
        "\n",
        "token_logits = model(**inputs).logits\n",
        "\n",
        "print (f\"token_logits:\\t{token_logits}\\n\")\n",
        "\n",
        "print (f\"tokenizer.mask_token_id:\\t{tokenizer.mask_token_id}\\n\")\n",
        "\n",
        "# Find the location of [MASK] and extract its logits\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "print (f\"mask_token_index:\\t{mask_token_index}\\n\")\n",
        "print (f\"mask_token_logits:\\t{mask_token_logits}\\n\")\n",
        "\n",
        "# Pick the [MASK] candidates with the highest logits\n",
        "top_10_tokens = torch.topk(mask_token_logits, 10, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_10_tokens:\n",
        "    print(f\">>> {frase.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46zW5x7Sn767",
        "outputId": "159aea2e-3b71-4973-d7f4-1915f99a09b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\t{'input_ids': tensor([[ 101, 2951, 2671,  103, 2097, 2025, 3413, 1996, 2607,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "\n",
            "tokenizer.decode([token]):\t[CLS] data science [MASK] will not pass the course [SEP]\n",
            "\n",
            "tokenizer.mask_token:\t[MASK]\n",
            "token_logits:\ttensor([[[ -6.7515,  -6.7122,  -6.7156,  ...,  -6.0969,  -5.8909,  -4.1559],\n",
            "         [ -9.3213,  -9.2264,  -9.1887,  ...,  -8.6386,  -7.7959,  -6.7946],\n",
            "         [ -9.2814,  -8.8226,  -8.9900,  ...,  -8.5409,  -7.7992, -10.2986],\n",
            "         ...,\n",
            "         [-14.2551, -14.1695, -13.6868,  ..., -11.5396, -10.7572, -12.0141],\n",
            "         [ -9.0199,  -8.9663,  -8.8720,  ...,  -8.1527,  -7.9353,  -7.4043],\n",
            "         [-13.0369, -12.8171, -13.0187,  ..., -10.1752, -11.7309,  -8.2591]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "tokenizer.mask_token_id:\t103\n",
            "\n",
            "mask_token_index:\ttensor([3])\n",
            "\n",
            "mask_token_logits:\ttensor([[-5.7791, -5.6853, -5.7119,  ..., -5.7060, -5.5290, -5.8119]],\n",
            "       grad_fn=<IndexBackward0>)\n",
            "\n",
            ">>> data science students will not pass the course\n",
            ">>> data science candidates will not pass the course\n",
            ">>> data science graduates will not pass the course\n",
            ">>> data science student will not pass the course\n",
            ">>> data science : will not pass the course\n",
            ">>> data science - will not pass the course\n",
            ">>> data science course will not pass the course\n",
            ">>> data science courses will not pass the course\n",
            ">>> data science teachers will not pass the course\n",
            ">>> data science i will not pass the course\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Múltiples mascaras"
      ],
      "metadata": {
        "id": "fHH8WETevHek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "3f_C4NrZvOkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84fe064-c43f-41bd-ec10-cfc3e783725f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get mask token\n",
        "mask = mlm.tokenizer.mask_token\n",
        "\n",
        "# frase = f'{mask} science {mask} will not pass the course'\n",
        "frase = f\"Using a Transformer {mask} is {mask}\"\n",
        "\n",
        "inputs = tokenizer(frase, return_tensors=\"pt\")\n",
        "\n",
        "tokenized_text = tokenizer.tokenize(frase)\n",
        "print (tokenized_text)\n",
        "\n",
        "print (inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0wOiUvdvTh0",
        "outputId": "98b05606-fab1-4f7f-89e6-528c40bb8314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['using', 'a', 'transform', '##er', '[MASK]', 'is', '[MASK]']\n",
            "{'input_ids': tensor([[  101,  2478,  1037, 10938,  2121,   103,  2003,   103,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "predictions = outputs[0]\n",
        "\n",
        "print (predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzW6xq26v7QN",
        "outputId": "c6ef1faf-d5bc-4981-a725-3a52a4ddd9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ -6.7083,  -6.6438,  -6.6441,  ...,  -6.0651,  -5.7368,  -3.8747],\n",
            "         [ -6.6019,  -6.3780,  -6.6740,  ...,  -6.5999,  -5.0723,  -4.1171],\n",
            "         [-13.4614, -13.3045, -13.3661,  ..., -12.4056,  -9.6751,  -9.4832],\n",
            "         ...,\n",
            "         [ -9.6787,  -9.4650,  -9.4854,  ...,  -9.3639,  -6.1835,  -8.5192],\n",
            "         [ -6.7329,  -6.4565,  -6.7158,  ...,  -6.6708,  -6.1550,  -2.9264],\n",
            "         [-10.4209, -10.0972, -10.2875,  ...,  -8.0917,  -8.6419,  -7.6760]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_preds, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
        "\n",
        "for k in range(10):\n",
        "    predicted_index = [sorted_idx[i, k].item() for i in range(0,len(tokenized_text))]\n",
        "    predicted_token = [tokenizer.convert_ids_to_tokens([predicted_index[x]])[0] for x in range(1,len(tokenized_text))]\n",
        "    print(predicted_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEF76Mc0wARh",
        "outputId": "648b899d-5125-4e62-90d0-7a5e96852479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['using', 'a', 'transform', '##er', 'that', 'is']\n",
            "['for', 'the', 'transforms', '-', ',', 'be']\n",
            "['with', 'an', 'convert', '##ian', 'it', ':']\n",
            "['use', 'any', 'sum', ':', 'where', 'for']\n",
            "['applying', 'this', 'radical', 'kernel', 'this', ',']\n",
            "['so', 'is', 'mean', 'transform', 'there', 'as']\n",
            "['taking', 'its', 'fourier', 'operator', ':', 'then']\n",
            "['then', 'with', 'grid', 'in', 'which', 'becomes']\n",
            "['in', 'another', 'transformation', ',', '.', 'and']\n",
            "['choosing', 'in', 'auxiliary', '*', 'function', 'if']\n"
          ]
        }
      ]
    }
  ]
}