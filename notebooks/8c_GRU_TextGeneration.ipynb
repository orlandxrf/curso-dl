{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8c_GRU_TextGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgtT/WnEQMX266Hd8Dv+Rp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orlandxrf/curso-dl/blob/main/notebooks/8c_GRU_TextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de texto usando una GRU"
      ],
      "metadata": {
        "id": "uqJ2YE-jhWsV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vd-Rpna4fiOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b71d900-d46e-4d27-f55c-e4d0f835f78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepath:\tdata/text_emotion.csv\n"
          ]
        }
      ],
      "source": [
        "# establecer parametros para almacenar y cargar el conjunto de datos del repositorio de Github\n",
        "import os\n",
        "\n",
        "URL = 'https://raw.githubusercontent.com/orlandxrf/curso-dl/main/data/text_emotion.csv'\n",
        "data_folder = 'data'\n",
        "filepath = os.path.join(data_folder, 'text_emotion.csv')\n",
        "print (f\"filepath:\\t{filepath}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar el conjunto de datos"
      ],
      "metadata": {
        "id": "YMEoN-nnhhGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# crear carpeta para almacenar el conjunto de datos\n",
        "! mkdir {data_folder}\n",
        "# descargar conjunto de datos y alamcenar\n",
        "! wget -nc {URL} -O {filepath}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sll0mlf_aszv",
        "outputId": "8537e004-51bc-42e1-ab4d-100ab27ae291"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "File ‘data/text_emotion.csv’ already there; not retrieving.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comprobrar\n",
        "! ls -lh data/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aV8rUjwbbBO",
        "outputId": "9b2ae09b-ee14-47d3-c462-605a0ac1dea8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 4.2M Mar  8 14:27 data/text_emotion.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analizar conjunto de datos"
      ],
      "metadata": {
        "id": "fbNBV1eaXjZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/text_emotion.csv')\n",
        "\n",
        "# eliminar columnas 'tweet_id' y 'author'\n",
        "df.drop('tweet_id',axis=1,inplace=True)\n",
        "df.drop('author',axis=1,inplace=True)\n",
        "print (df.head(10))\n",
        "print (f\"Longitud de tweets: {len(df):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pijhdpXRXnAs",
        "outputId": "61d57566-14e6-474c-efa6-3b4efb840eb2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    sentiment                                            content\n",
            "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
            "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
            "2     sadness                Funeral ceremony...gloomy friday...\n",
            "3  enthusiasm               wants to hang out with friends SOON!\n",
            "4     neutral  @dannycastillo We want to trade with someone w...\n",
            "5       worry  Re-pinging @ghostridah14: why didn't you go to...\n",
            "6     sadness  I should be sleep, but im not! thinking about ...\n",
            "7       worry               Hmmm. http://www.djhero.com/ is down\n",
            "8     sadness            @charviray Charlene my love. I miss you\n",
            "9     sadness         @kelcouch I'm sorry  at least it's Friday?\n",
            "Longitud de tweets: 40,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentiments = dict(pd.value_counts(df['sentiment']))\n",
        "classes = list(sentiments.keys())\n",
        "for i, sent in enumerate(sentiments):\n",
        "  print (f\"{i+1}\\t{sentiments[sent]}\\t{sent}\")\n",
        "print (f\"\\n{len(df):,} tweets\")\n",
        "\n",
        "# Graficar la distribución\n",
        "plt.figure(figsize=(20, 10))\n",
        "pd.value_counts(df['sentiment']).plot.bar(title=\"Distribución de sentimientos\")\n",
        "plt.xlabel(\"Sentimiento\", fontsize=\"15\")\n",
        "plt.ylabel(\"Tweets\", fontsize=\"15\")\n",
        "plt.xticks(rotation=45, fontsize=\"15\")\n",
        "plt.yticks(fontsize=\"15\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "rJifibVZYINg",
        "outputId": "b10ae653-f807-47ff-8135-95ff60512d76"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t8638\tneutral\n",
            "2\t8459\tworry\n",
            "3\t5209\thappiness\n",
            "4\t5165\tsadness\n",
            "5\t3842\tlove\n",
            "6\t2187\tsurprise\n",
            "7\t1776\tfun\n",
            "8\t1526\trelief\n",
            "9\t1323\thate\n",
            "10\t827\tempty\n",
            "11\t759\tenthusiasm\n",
            "12\t179\tboredom\n",
            "13\t110\tanger\n",
            "\n",
            "40,000 tweets\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento de los datos\n",
        "Realizar el preprocesamiento necesario, que incluye la limpieza y otros elementos como la toeknización y lematización\n",
        "\n",
        "## Limpieza de los tweets"
      ],
      "metadata": {
        "id": "MGucn7IxiJZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocesamiento\n",
        "import numpy as np\n",
        "def limpiezaDeTweets(tweet):\n",
        "  import re\n",
        "  tweet = re.sub(r'RT @\\w+: ',\" \", tweet) # eliminar retweets\n",
        "  tweet = re.sub(r'https?://[A-Za-z0-9./]+', ' ', tweet) # eliminar urls\n",
        "  tweet = re.sub(r'@[A-Za-z0-9]+', '', tweet) # eliminar usuarios\n",
        "  tweet = tweet.lower() # convertir a minúsculas\n",
        "  tweet = re.sub(r'[^a-z0-9\\'\\.,:; ]+', ' ', tweet) # eliminar todos los caracteres que no esten en los definidos\n",
        "  tweet = ' '.join(tweet.split()) # eliminar múltiples espacios, solo conservar un espacio\n",
        "\n",
        "  return tweet\n",
        "\n",
        "\n",
        "df['content'] = df['content'].astype('str')\n",
        "df['content'] = df['content'].apply(lambda x: limpiezaDeTweets(x))\n",
        "\n",
        "df.drop_duplicates(keep=False, inplace=True)\n",
        "\n",
        "df = df.replace(r'^\\s*$', np.NaN, regex=True)\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "print (df.head(10))\n",
        "print (f\"\\nTotal:\\t{len(df):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26671a85-0792-4082-8ffa-06247ff6bcc1",
        "id": "9smcPkBDiNjc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    sentiment                                            content\n",
            "0       empty  i know i was listenin to bad habit earlier and...\n",
            "1     sadness  layin n bed with a headache ughhhh...waitin on...\n",
            "2     sadness                funeral ceremony...gloomy friday...\n",
            "3  enthusiasm                wants to hang out with friends soon\n",
            "4     neutral  we want to trade with someone who has houston ...\n",
            "5       worry  re pinging : why didn't you go to prom bc my b...\n",
            "6     sadness  i should be sleep, but im not thinking about a...\n",
            "7       worry                                      hmmm. is down\n",
            "8     sadness                       charlene my love. i miss you\n",
            "9     sadness                     i'm sorry at least it's friday\n",
            "\n",
            "Total:\t39,314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mostrar los tweets limpios por clase\n",
        "sentiments = dict(pd.value_counts(df['sentiment']))\n",
        "classes = list(sentiments.keys())\n",
        "for i, sent in enumerate(sentiments):\n",
        "  print (f\"{i+1}\\t{sentiments[sent]}\\t{sent}\")\n",
        "print (f\"\\n{len(df):,} tweets\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBDh3G5VrqxO",
        "outputId": "5405acf0-8e0f-4954-d3d5-ae68c481a5f9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t8380\tneutral\n",
            "2\t8374\tworry\n",
            "3\t5126\tsadness\n",
            "4\t5099\thappiness\n",
            "5\t3695\tlove\n",
            "6\t2181\tsurprise\n",
            "7\t1774\tfun\n",
            "8\t1514\trelief\n",
            "9\t1320\thate\n",
            "10\t805\tempty\n",
            "11\t757\tenthusiasm\n",
            "12\t179\tboredom\n",
            "13\t110\tanger\n",
            "\n",
            "39,314 tweets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtener solo la clase `happiness` como datos de entrada del modelo"
      ],
      "metadata": {
        "id": "-gPZ-fUgi37N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "happiness = df[df['sentiment'] == 'happiness']\n",
        "tweets = happiness['content'].to_list()\n",
        "\n",
        "print (f\"\\nLongitud de los tweets tweets: {len(tweets):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_wEPQShjAaB",
        "outputId": "d6a06b1f-909b-48ed-ec09-622f2233b270"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Longitud de los tweets tweets: 5,099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de un solo tweet\n",
        "Primero construiremos una red neuronal que pueda generar un tweet muy bien. Podemos elegir cualquier tweet (o cualquier otro texto) que queramos. Elijamos construir un RNN que genere texto a partir del `tweet[2000]`."
      ],
      "metadata": {
        "id": "YaACslpRkMnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = tweets[2000]\n",
        "print (tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAuLJM93oSds",
        "outputId": "dca4b66e-e088-44eb-f84b-56c58cea8964"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes... today is star wars day may the 4th be with you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 Codificación\n",
        "Codificar el tweet usando una codificación one-hot. Crearemos asignaciones de diccionario desde el carácter hasta el índice de ese carácter (un identificador entero único) y desde el índice hasta el carácter. Usaremos el mismo esquema de nombres que usa torchtext (stoi e itos).\n",
        "\n",
        "El vocabulario es limitado ya que contiene solo los caracteres del `tweet[2000]`, más dos tokens especiales:\n",
        "\n",
        "*   `<EOS>` representa el \"fin de la cadena\" (EOS: End Of Sentence), que agregaremos al final de nuestro tweet. Dado que los tweets son de longitud variable, esta es una forma en que GRU indica que se ha generado la secuencia completa.\n",
        "*   `<BOS>` representa \"comienzo de cadena\" (BOS: Beginning Of Sentence), que agregaremos al comienzo de nuestro tweet. Este es el primer token que introduciremos en la GRU.\n",
        "\n"
      ],
      "metadata": {
        "id": "0Zb_WP_DiuNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(tweet)) + [\"<BOS>\", \"<EOS>\"]\n",
        "vocab_stoi = {s: i for i, s in enumerate(vocab)}\n",
        "vocab_itos = {i: s for i, s in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print (f\"longitud tweet:\\t{len(tweet)}\")\n",
        "print (f\"\\nvocabulario:\\t{vocab}\")\n",
        "print (f\"\\nvocab_stoi: \\t{vocab_stoi}\") # vocabulario stoi: string to index\n",
        "print (f\"\\nvocab_itos: \\t{vocab_itos}\") # vocabulario itos: index to string\n",
        "print (f\"\\nvocab_size: \\t{vocab_size}\") # longitud del vocabulario\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBOo3BXrtwi5",
        "outputId": "2c2fa288-8b45-4d10-9692-849cc69cdc77"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "longitud tweet:\t53\n",
            "\n",
            "vocabulario:\t['a', '4', '.', 'e', 'i', 'd', 'r', 's', 'm', 'y', 'b', ' ', 't', 'o', 'u', 'w', 'h', '<BOS>', '<EOS>']\n",
            "\n",
            "vocab_stoi: \t{'a': 0, '4': 1, '.': 2, 'e': 3, 'i': 4, 'd': 5, 'r': 6, 's': 7, 'm': 8, 'y': 9, 'b': 10, ' ': 11, 't': 12, 'o': 13, 'u': 14, 'w': 15, 'h': 16, '<BOS>': 17, '<EOS>': 18}\n",
            "\n",
            "vocab_itos: \t{0: 'a', 1: '4', 2: '.', 3: 'e', 4: 'i', 5: 'd', 6: 'r', 7: 's', 8: 'm', 9: 'y', 10: 'b', 11: ' ', 12: 't', 13: 'o', 14: 'u', 15: 'w', 16: 'h', 17: '<BOS>', 18: '<EOS>'}\n",
            "\n",
            "vocab_size: \t19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Modelo\n",
        "Diseño del modelo de la red GRU"
      ],
      "metadata": {
        "id": "NFJDLrcbv67N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
        "        super(TextGenerator, self).__init__()\n",
        "\n",
        "        # matriz de identidad para generar vectores one-hot\n",
        "        self.ident = torch.eye(vocab_size)\n",
        "\n",
        "        # recurrent neural network (Gated Recurrent Unit)\n",
        "        self.rnn = nn.GRU(vocab_size, hidden_size, n_layers, batch_first=True)\n",
        "\n",
        "        # una capa totalmente conectada (FC) que genera una distribución sobre el siguiente token, dada la salida RNN\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, input, hidden=None):\n",
        "        # generar vectores one-hot de entrada\n",
        "        input = self.ident[input]\n",
        "\n",
        "        # obtener la siguiente salida y el estado oculto\n",
        "        output, hidden = self.rnn(input, hidden)\n",
        "\n",
        "        # predecir la distribución sobre los siguientes tokens\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "LeJIUlv5wIv7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciar el modelo\n",
        "model = TextGenerator(vocab_size, 64)\n",
        "print (model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M25WooNxxvtG",
        "outputId": "ff47493c-e80d-4951-8c75-c192ae78654f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextGenerator(\n",
            "  (rnn): GRU(19, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=19, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 Entrenamiento\n",
        "Un modelo RNN genera texto un carácter a la vez en función del valor de estado oculto.\n",
        "\n",
        "En cada paso de tiempo, comprueba si el modelo generó el carácter correcto.\n",
        "\n",
        "Es decir, en cada paso de tiempo, está tratando de seleccionar el siguiente carácter correcto entre todos los caracteres del vocabulario.\n",
        "\n",
        "Recordar que este problema es un problema de clasificación de clases múltiples, por lo que se usa la pérdida de entropía cruzada para entrenar la red."
      ],
      "metadata": {
        "id": "tLzLf0CGyss9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# función de pérdida\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "cjJ-5eUVzQVI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para comprender lo que sucede durante el entrenamiento de la RNN, iniciamos con un entrenamiento ineficiente, pero muestra lo que sucede paso a paso.\n",
        "\n",
        "Calcular la pérdida del primer token generado, luego la del segundo token y así sucesivamente.\n",
        "\n",
        "![Forward](https://drive.google.com/uc?id=1UJ9GajU6-iS7erkx5ppWgnZQWmIWc1V9)\n",
        "\n",
        "Iniciamos generando el primer token (`tweet[0] = <BOS>`).\n",
        "\n"
      ],
      "metadata": {
        "id": "Gxov91hJzZLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# para generar el primer token, se alimenta la red RNN (con un estado oculto vacío al inicio)\n",
        "# con el token \"<BOS>\", así se obtiene la primer salida\n",
        "\n",
        "bos_input = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long()\n",
        "print (f\"bos_input:\\t{bos_input}\\nshape:\\t{bos_input.shape}\")\n",
        "\n",
        "bos_input = bos_input.unsqueeze(0) # agregar una dimensión\n",
        "print (f\"\\nbos_input:\\t{bos_input}\\nshape:\\t{bos_input.shape}\")\n",
        "\n",
        "# estado oculto vacío al inicio\n",
        "output, hidden = model(bos_input, hidden=None)\n",
        "\n",
        "# distribución obtenida sobre el primer token\n",
        "print (f\"output:\\t{output}\\n\\noutput shape:\\t{output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_SP7bvh3PxE",
        "outputId": "641dd082-d044-4a64-dfc4-923a11e47759"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bos_input:\ttensor([17])\n",
            "shape:\ttorch.Size([1])\n",
            "\n",
            "bos_input:\ttensor([[17]])\n",
            "shape:\ttorch.Size([1, 1])\n",
            "output:\ttensor([[[-0.0580, -0.0144, -0.0377,  0.0196, -0.0316, -0.0396, -0.1033,\n",
            "          -0.1084,  0.0891, -0.0275, -0.0976, -0.0287,  0.0011,  0.0506,\n",
            "           0.1559, -0.0459,  0.0391,  0.0461, -0.0847]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "output shape:\ttorch.Size([1, 1, 19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, calcular la pérdida usando `loss_function`. Dado que el modelo no está entrenado, se espera que la pérdida sea alta. (Por ahora, no haremos nada con esta pérdida y omitiremos el pase hacia atrás)."
      ],
      "metadata": {
        "id": "Og6i6W6w-TEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos el target\n",
        "print (f\"vocab_stoi[tweet[0]]:\\t{vocab_stoi[tweet[0]]}\\t{tweet[0]}\")\n",
        "\n",
        "# convertir el id de a tensor con dos dimensiones\n",
        "target = torch.Tensor([vocab_stoi[tweet[0]]]).long().unsqueeze(0)\n",
        "print (f\"\\ntarget:\\t{target.shape}\")\n",
        "\n",
        "# reshaping la salida a un tensor 2D\n",
        "output = output.reshape(-1, vocab_size)\n",
        "print (f\"\\noutput:\\t{output.shape}\")\n",
        "\n",
        "# aplicar la función de pérdida, redimensionar el target a un tensor 1D \n",
        "loss = loss_function(output, target.reshape(-1))\n",
        "\n",
        "print (f\"\\nloss:\\t{loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE-bHyxW-dSQ",
        "outputId": "8e6324fc-b099-4a35-e6e5-c974a8ca19bc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_stoi[tweet[0]]:\t9\ty\n",
            "\n",
            "target:\ttorch.Size([1, 1])\n",
            "\n",
            "output:\ttorch.Size([1, 19])\n",
            "\n",
            "loss:\t2.959717035293579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo siguiente es actualizar el estado oculto y generar una predicción para el siguiente token.\n",
        "\n",
        "Para hacerlo, se debe proporcionar el token actual a la RNN.\n",
        "\n",
        "Durante el tiempo de prueba, se necesita tomar muestras de la probabilidad predicha sobre los tokens que acaba de generar la red neuronal.\n",
        "\n",
        "Sin embargo se puede hacer algo mejor: usar el token objetivo real (**the ground-truth or actual target token**). \n",
        "\n",
        "Esta técnica se llama **teacher-forcing** y generalmente acelera el entrenamiento.\n",
        "\n",
        "La razón es que en este momento, dado que el modelo no funciona bien, la distribución de probabilidad predicha está bastante lejos del target objetivo real.\n",
        "\n",
        "Por lo tanto, es bastante difícil para la red neuronal volver a entrenar con datos de entrada incorrectos."
      ],
      "metadata": {
        "id": "MTADuMBpFvjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usar el teacher-forcing: pasamos en el token objetivo real (target), en lugar de usar la distribución predicha de la RNN\n",
        "\n",
        "output, hidden = model(target, hidden)\n",
        "\n",
        "print (f\"output:\\t{output}\\n\\n{output.shape}\") # distribución obtenida del segundo token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uwgvUVnHkVq",
        "outputId": "30916b49-4433-47fd-aab0-91ee17fc6d78"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output:\ttensor([[[-0.0356, -0.0222,  0.0063,  0.0388, -0.0333, -0.1093, -0.1964,\n",
            "          -0.0841,  0.0677, -0.0425, -0.0589, -0.0040, -0.0296,  0.0609,\n",
            "           0.1406, -0.0336,  0.0830,  0.0266, -0.0399]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "torch.Size([1, 1, 19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De forma similar al primer paso, calcular la pérdida, cuantificando la diferencia entre la distribución prevista y el siguiente token objetivo real.\n",
        "\n",
        "Esta pérdida se puede utilizar para ajustar los pesos de la red neuronal (que aún no se está haciendo)."
      ],
      "metadata": {
        "id": "uCkaqsNwJJts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos el segundo target\n",
        "print (f\"vocab_stoi[tweet[1]]:\\t{vocab_stoi[tweet[1]]}\\t{tweet[1]}\")\n",
        "\n",
        "# convertir el target siguiente \"tweet[1]\" a tensores 2D\n",
        "target = torch.Tensor([vocab_stoi[tweet[1]]]).long().unsqueeze(0)\n",
        "\n",
        "# reshaping la salida a un tensor 2D\n",
        "output = output.reshape(-1, vocab_size)\n",
        "print (f\"\\noutput:\\t{output.shape}\")\n",
        "\n",
        "# aplicar la función de pérdida, redimensionar el target a un tensor 1D \n",
        "loss = loss_function(output, target.reshape(-1))\n",
        "\n",
        "print (f\"\\nloss:\\t{loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMFDFKZOJ6IM",
        "outputId": "36845c67-c668-4d18-8794-e4683687efac"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_stoi[tweet[1]]:\t3\te\n",
            "\n",
            "output:\ttorch.Size([1, 19])\n",
            "\n",
            "loss:\t2.8943402767181396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos continuar este proceso de: \n",
        "\n",
        "*   alimentar el token objetivo real a la RNN,\n",
        "*   obtener la distribución de predicción sobre el siguiente token,\n",
        "*   y calcular la pérdida\n",
        "\n",
        "esto se hace tantos pasos como tokens haya en el tweet **ground-truth**."
      ],
      "metadata": {
        "id": "BITgTo72KnJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definir un ciclo for que realice los cálculos del resto de tokens del tweet\n",
        "for i in range(2, len(tweet)): # inicializar en 2, ya se cálculo el 0 y 1\n",
        "    # target anterior\n",
        "    output, hidden = model(target, hidden)\n",
        "     # target actual\n",
        "    target = torch.Tensor([vocab_stoi[tweet[i]]]).long().unsqueeze(0)\n",
        "    # redimensionar output a un tensor 2D, redimensionar el target a un tensor 1D y calcular la pérdida\n",
        "    loss = loss_function(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "    \n",
        "    print(f\"i: {i} char: {tweet[i]}\\noutput: {output}\\nloss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5e9WrFGLdEP",
        "outputId": "677cfce0-098c-4e70-cd26-54ad10000c0c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i: 2 char: s\n",
            "output: tensor([[[-0.0921, -0.0021,  0.0217,  0.0107,  0.0045, -0.1121, -0.1694,\n",
            "          -0.1011,  0.0593, -0.0607, -0.0581, -0.0471, -0.0266,  0.0600,\n",
            "           0.1342, -0.0138,  0.0911,  0.0177, -0.0293]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.0316884517669678\n",
            "i: 3 char: .\n",
            "output: tensor([[[-0.0578, -0.0208,  0.0219,  0.0189, -0.0169, -0.1005, -0.2171,\n",
            "          -0.0723,  0.0621, -0.0597, -0.1029, -0.0732, -0.0366,  0.0620,\n",
            "           0.1034, -0.0442,  0.0738, -0.0012, -0.0229]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.899763584136963\n",
            "i: 4 char: .\n",
            "output: tensor([[[-0.0295, -0.0319,  0.0135,  0.0220, -0.0561, -0.0948, -0.1997,\n",
            "          -0.0891,  0.0814, -0.0772, -0.0793, -0.0431, -0.0921,  0.0744,\n",
            "           0.0678, -0.0799,  0.0807,  0.0100, -0.0194]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.904967784881592\n",
            "i: 5 char: .\n",
            "output: tensor([[[-0.0167, -0.0328,  0.0074,  0.0259, -0.0779, -0.0902, -0.1852,\n",
            "          -0.0989,  0.0934, -0.0888, -0.0654, -0.0297, -0.1155,  0.0795,\n",
            "           0.0491, -0.0924,  0.0838,  0.0189, -0.0113]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9109673500061035\n",
            "i: 6 char:  \n",
            "output: tensor([[[-0.0104, -0.0318,  0.0035,  0.0291, -0.0904, -0.0875, -0.1750,\n",
            "          -0.1045,  0.1008, -0.0969, -0.0569, -0.0238, -0.1252,  0.0816,\n",
            "           0.0387, -0.0962,  0.0850,  0.0254, -0.0048]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.942689895629883\n",
            "i: 7 char: t\n",
            "output: tensor([[[ 0.0026, -0.0850,  0.0252,  0.0262, -0.0496, -0.0611, -0.1539,\n",
            "          -0.0766,  0.0776, -0.0643, -0.0801, -0.0434, -0.1153,  0.0842,\n",
            "           0.0338, -0.0431,  0.0716,  0.0370, -0.0307]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.038533926010132\n",
            "i: 8 char: o\n",
            "output: tensor([[[-0.0192, -0.1048,  0.0296,  0.0613, -0.0580, -0.0802, -0.1958,\n",
            "          -0.1100,  0.0798, -0.0873, -0.0759,  0.0202, -0.0768,  0.0832,\n",
            "           0.0890, -0.0461,  0.0938,  0.0138, -0.0383]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.8421852588653564\n",
            "i: 9 char: d\n",
            "output: tensor([[[-0.0438, -0.0584, -0.0112,  0.0550, -0.0641, -0.0950, -0.1993,\n",
            "          -0.1153,  0.0543, -0.0578, -0.0603,  0.0115, -0.0853,  0.0732,\n",
            "           0.1192, -0.0226,  0.0897,  0.0300, -0.0483]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.019913911819458\n",
            "i: 10 char: a\n",
            "output: tensor([[[-0.0830, -0.0466,  0.0317,  0.0067, -0.0474, -0.0705, -0.1724,\n",
            "          -0.0988,  0.0988, -0.0395, -0.0964, -0.0325, -0.0872,  0.0555,\n",
            "           0.1036, -0.0059,  0.0644,  0.0557, -0.0301]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.009321451187134\n",
            "i: 11 char: y\n",
            "output: tensor([[[-0.0872, -0.0281,  0.0280,  0.0028, -0.0450, -0.0547, -0.1696,\n",
            "          -0.0983,  0.0948, -0.0036, -0.1163, -0.0580, -0.1160,  0.1136,\n",
            "           0.1261, -0.0440,  0.0892,  0.0183, -0.0598]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9298665523529053\n",
            "i: 12 char:  \n",
            "output: tensor([[[-0.0446, -0.0374,  0.0469,  0.0295, -0.0411, -0.1152, -0.2335,\n",
            "          -0.0799,  0.0636, -0.0268, -0.0715, -0.0158, -0.1087,  0.0992,\n",
            "           0.1226, -0.0317,  0.1088,  0.0063, -0.0298]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9448318481445312\n",
            "i: 13 char: i\n",
            "output: tensor([[[-0.0213, -0.0873,  0.0571,  0.0243, -0.0238, -0.0731, -0.1911,\n",
            "          -0.0576,  0.0525, -0.0203, -0.0888, -0.0367, -0.1099,  0.0941,\n",
            "           0.0767, -0.0181,  0.0888,  0.0202, -0.0463]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.951890468597412\n",
            "i: 14 char: s\n",
            "output: tensor([[[-2.2784e-02, -9.7545e-02,  2.4399e-02,  4.3674e-02,  1.2242e-04,\n",
            "          -7.9208e-02, -2.0071e-01, -5.0833e-02,  3.8680e-02, -3.4771e-02,\n",
            "          -6.3573e-02, -2.1976e-02, -9.6505e-02,  1.1137e-01,  9.2252e-02,\n",
            "           1.6053e-02,  6.4913e-02,  1.4646e-02, -4.1973e-02]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.981904983520508\n",
            "i: 15 char:  \n",
            "output: tensor([[[-0.0262, -0.0674,  0.0257,  0.0325, -0.0282, -0.0893, -0.2316,\n",
            "          -0.0411,  0.0478, -0.0525, -0.1043, -0.0538, -0.0714,  0.0842,\n",
            "           0.0842, -0.0294,  0.0649, -0.0006, -0.0305]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9752817153930664\n",
            "i: 16 char: s\n",
            "output: tensor([[[-0.0066, -0.1105,  0.0430,  0.0235, -0.0201, -0.0635, -0.1918,\n",
            "          -0.0371,  0.0450, -0.0383, -0.1073, -0.0470, -0.0938,  0.0829,\n",
            "           0.0600, -0.0192,  0.0658,  0.0182, -0.0522]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9602279663085938\n",
            "i: 17 char: t\n",
            "output: tensor([[[-0.0145, -0.0713,  0.0331,  0.0245, -0.0326, -0.0838, -0.2264,\n",
            "          -0.0344,  0.0503, -0.0529, -0.1281, -0.0700, -0.0701,  0.0706,\n",
            "           0.0707, -0.0479,  0.0669,  0.0010, -0.0376]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9880361557006836\n",
            "i: 18 char: a\n",
            "output: tensor([[[-0.0244, -0.1043,  0.0376,  0.0605, -0.0488, -0.0946, -0.2401,\n",
            "          -0.0870,  0.0614, -0.0792, -0.1041,  0.0172, -0.0623,  0.0786,\n",
            "           0.1109, -0.0528,  0.0929, -0.0090, -0.0496]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.946227550506592\n",
            "i: 19 char: r\n",
            "output: tensor([[[-0.0530, -0.0523,  0.0333,  0.0264, -0.0434, -0.0712, -0.2067,\n",
            "          -0.0881,  0.0732, -0.0274, -0.1236, -0.0291, -0.1026,  0.1179,\n",
            "           0.1279, -0.0675,  0.0997, -0.0087, -0.0697]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.1302242279052734\n",
            "i: 20 char:  \n",
            "output: tensor([[[-0.0369, -0.0732,  0.0243,  0.0084, -0.0390, -0.0452, -0.1876,\n",
            "          -0.0996,  0.0603, -0.0041, -0.1435, -0.0381, -0.0868,  0.1293,\n",
            "           0.1092, -0.0502,  0.0648, -0.0162, -0.0757]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.959340810775757\n",
            "i: 21 char: w\n",
            "output: tensor([[[-0.0152, -0.1083,  0.0521,  0.0125, -0.0209, -0.0391, -0.1660,\n",
            "          -0.0686,  0.0547, -0.0084, -0.1325, -0.0513, -0.1055,  0.1064,\n",
            "           0.0719, -0.0272,  0.0674,  0.0091, -0.0679]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9513015747070312\n",
            "i: 22 char: a\n",
            "output: tensor([[[-0.0746, -0.0673,  0.0560, -0.0120, -0.0235, -0.0610, -0.1624,\n",
            "          -0.0648,  0.0776, -0.0260, -0.1417, -0.0491, -0.1271,  0.1240,\n",
            "           0.1127, -0.0208,  0.0865, -0.0047, -0.0456]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.999948263168335\n",
            "i: 23 char: r\n",
            "output: tensor([[[-0.0840, -0.0366,  0.0389, -0.0112, -0.0320, -0.0535, -0.1601,\n",
            "          -0.0804,  0.0857, -0.0030, -0.1491, -0.0698, -0.1298,  0.1450,\n",
            "           0.1375, -0.0463,  0.0952, -0.0133, -0.0645]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.0856285095214844\n",
            "i: 24 char: s\n",
            "output: tensor([[[-0.0568, -0.0661,  0.0259, -0.0110, -0.0333, -0.0360, -0.1603,\n",
            "          -0.0980,  0.0682,  0.0070, -0.1594, -0.0622, -0.0986,  0.1446,\n",
            "           0.1197, -0.0365,  0.0614, -0.0215, -0.0712]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.0202417373657227\n",
            "i: 25 char:  \n",
            "output: tensor([[[-0.0448, -0.0503,  0.0349,  0.0075, -0.0357, -0.0695, -0.2100,\n",
            "          -0.0707,  0.0653, -0.0277, -0.1615, -0.0811, -0.0839,  0.1025,\n",
            "           0.1022, -0.0523,  0.0626, -0.0228, -0.0422]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.998202085494995\n",
            "i: 26 char: d\n",
            "output: tensor([[[-0.0173, -0.1003,  0.0546,  0.0120, -0.0206, -0.0552, -0.1810,\n",
            "          -0.0524,  0.0529, -0.0236, -0.1403, -0.0658, -0.1056,  0.0928,\n",
            "           0.0708, -0.0316,  0.0645,  0.0043, -0.0555]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9760663509368896\n",
            "i: 27 char: a\n",
            "output: tensor([[[-0.0657, -0.0655,  0.0615, -0.0106, -0.0179, -0.0548, -0.1661,\n",
            "          -0.0645,  0.0907, -0.0213, -0.1396, -0.0751, -0.0926,  0.0653,\n",
            "           0.0814, -0.0126,  0.0508,  0.0363, -0.0321]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9898884296417236\n",
            "i: 28 char: y\n",
            "output: tensor([[[-0.0763, -0.0365,  0.0408, -0.0034, -0.0266, -0.0492, -0.1701,\n",
            "          -0.0797,  0.0847,  0.0046, -0.1391, -0.0807, -0.1147,  0.1176,\n",
            "           0.1164, -0.0479,  0.0803,  0.0061, -0.0595]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9202804565429688\n",
            "i: 29 char:  \n",
            "output: tensor([[[-0.0378, -0.0403,  0.0527,  0.0280, -0.0300, -0.1135, -0.2358,\n",
            "          -0.0703,  0.0554, -0.0228, -0.0831, -0.0282, -0.1059,  0.1005,\n",
            "           0.1186, -0.0347,  0.1031, -0.0013, -0.0288]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.956406831741333\n",
            "i: 30 char: m\n",
            "output: tensor([[[-0.0172, -0.0878,  0.0594,  0.0245, -0.0173, -0.0730, -0.1937,\n",
            "          -0.0528,  0.0462, -0.0182, -0.0945, -0.0431, -0.1070,  0.0943,\n",
            "           0.0754, -0.0200,  0.0850,  0.0156, -0.0453]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.8813586235046387\n",
            "i: 31 char: a\n",
            "output: tensor([[[-0.0679, -0.0282,  0.0442,  0.0059, -0.0061, -0.0847, -0.2035,\n",
            "          -0.0322,  0.0603, -0.0444, -0.1059, -0.0973, -0.0549,  0.0918,\n",
            "           0.0919, -0.0119,  0.0596,  0.0019, -0.0252]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9935061931610107\n",
            "i: 32 char: y\n",
            "output: tensor([[[-0.0729, -0.0169,  0.0388,  0.0007, -0.0208, -0.0659, -0.1878,\n",
            "          -0.0676,  0.0744, -0.0098, -0.1247, -0.0923, -0.0875,  0.1232,\n",
            "           0.1232, -0.0442,  0.0820, -0.0076, -0.0540]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9359259605407715\n",
            "i: 33 char:  \n",
            "output: tensor([[[-0.0331, -0.0309,  0.0550,  0.0276, -0.0269, -0.1223, -0.2452,\n",
            "          -0.0662,  0.0529, -0.0303, -0.0762, -0.0338, -0.0887,  0.1002,\n",
            "           0.1234, -0.0311,  0.1035, -0.0059, -0.0256]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9631097316741943\n",
            "i: 34 char: t\n",
            "output: tensor([[[-0.0135, -0.0835,  0.0623,  0.0227, -0.0164, -0.0775, -0.1984,\n",
            "          -0.0524,  0.0468, -0.0220, -0.0911, -0.0454, -0.0965,  0.0925,\n",
            "           0.0788, -0.0179,  0.0855,  0.0145, -0.0431]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.0249054431915283\n",
            "i: 35 char: h\n",
            "output: tensor([[[-2.9561e-02, -1.0457e-01,  5.2435e-02,  5.7609e-02, -4.0433e-02,\n",
            "          -8.7057e-02, -2.2144e-01, -9.6436e-02,  6.1830e-02, -6.2166e-02,\n",
            "          -8.2015e-02,  1.9432e-02, -6.8110e-02,  8.9211e-02,  1.1423e-01,\n",
            "          -3.6843e-02,  1.0271e-01, -1.8539e-04, -4.5498e-02]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.825352191925049\n",
            "i: 36 char: e\n",
            "output: tensor([[[-0.0428, -0.0505,  0.0598,  0.0366, -0.0563, -0.1103, -0.1988,\n",
            "          -0.1233,  0.0686, -0.0406, -0.0893, -0.0221, -0.0764,  0.0655,\n",
            "           0.1405, -0.0463,  0.1172,  0.0176, -0.0485]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.8903419971466064\n",
            "i: 37 char:  \n",
            "output: tensor([[[-0.0895, -0.0216,  0.0549,  0.0049, -0.0087, -0.1157, -0.1745,\n",
            "          -0.1224,  0.0617, -0.0583, -0.0719, -0.0523, -0.0667,  0.0690,\n",
            "           0.1363, -0.0257,  0.1152,  0.0136, -0.0348]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.979605197906494\n",
            "i: 38 char: 4\n",
            "output: tensor([[[-0.0378, -0.0901,  0.0572,  0.0087, -0.0018, -0.0678, -0.1623,\n",
            "          -0.0797,  0.0578, -0.0328, -0.0892, -0.0546, -0.0913,  0.0808,\n",
            "           0.0840, -0.0174,  0.0909,  0.0229, -0.0478]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.017425060272217\n",
            "i: 39 char: t\n",
            "output: tensor([[[-0.0126, -0.0886,  0.0320, -0.0044,  0.0315, -0.0462, -0.1707,\n",
            "          -0.0810,  0.0634, -0.0295, -0.1018, -0.0525, -0.1211,  0.1053,\n",
            "           0.0991,  0.0067,  0.0672,  0.0016, -0.0396]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.0502548217773438\n",
            "i: 40 char: h\n",
            "output: tensor([[[-0.0299, -0.1079,  0.0328,  0.0461, -0.0105, -0.0662, -0.2117,\n",
            "          -0.1056,  0.0646, -0.0659, -0.0930,  0.0087, -0.0838,  0.0978,\n",
            "           0.1218, -0.0237,  0.0892, -0.0084, -0.0403]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.838261842727661\n",
            "i: 41 char:  \n",
            "output: tensor([[[-0.0431, -0.0510,  0.0477,  0.0321, -0.0378, -0.0979, -0.1979,\n",
            "          -0.1272,  0.0668, -0.0424, -0.0980, -0.0314, -0.0865,  0.0705,\n",
            "           0.1431, -0.0402,  0.1085,  0.0127, -0.0433]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9574127197265625\n",
            "i: 42 char: b\n",
            "output: tensor([[[-0.0159, -0.0961,  0.0563,  0.0208, -0.0194, -0.0684, -0.1774,\n",
            "          -0.0813,  0.0593, -0.0287, -0.1001, -0.0424, -0.1053,  0.0817,\n",
            "           0.0885, -0.0278,  0.0923,  0.0264, -0.0520]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.026705265045166\n",
            "i: 43 char: e\n",
            "output: tensor([[[-0.0261, -0.0817, -0.0010,  0.0389, -0.0113, -0.0306, -0.1654,\n",
            "          -0.0826,  0.0700, -0.0359, -0.0824, -0.0102, -0.1240,  0.0979,\n",
            "           0.1050, -0.0700,  0.0930,  0.0179, -0.0762]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.8885891437530518\n",
            "i: 44 char:  \n",
            "output: tensor([[[-0.0858, -0.0330,  0.0108,  0.0085,  0.0186, -0.0696, -0.1483,\n",
            "          -0.0939,  0.0608, -0.0600, -0.0756, -0.0545, -0.0791,  0.0766,\n",
            "           0.1206, -0.0344,  0.0970,  0.0143, -0.0479]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9816572666168213\n",
            "i: 45 char: w\n",
            "output: tensor([[[-0.0378, -0.0931,  0.0295,  0.0134,  0.0134, -0.0420, -0.1458,\n",
            "          -0.0624,  0.0561, -0.0376, -0.0942, -0.0605, -0.0925,  0.0815,\n",
            "           0.0779, -0.0192,  0.0793,  0.0231, -0.0543]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9465107917785645\n",
            "i: 46 char: i\n",
            "output: tensor([[[-0.0848, -0.0594,  0.0349, -0.0119, -0.0040, -0.0586, -0.1532,\n",
            "          -0.0616,  0.0808, -0.0410, -0.1206, -0.0556, -0.1155,  0.1113,\n",
            "           0.1160, -0.0161,  0.0899,  0.0026, -0.0378]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.931126356124878\n",
            "i: 47 char: t\n",
            "output: tensor([[[-0.0594, -0.0872,  0.0071,  0.0227,  0.0078, -0.0718, -0.1812,\n",
            "          -0.0569,  0.0598, -0.0455, -0.0871, -0.0291, -0.1023,  0.1230,\n",
            "           0.1179,  0.0224,  0.0637,  0.0015, -0.0355]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 3.0321805477142334\n",
            "i: 48 char: h\n",
            "output: tensor([[[-0.0591, -0.1141,  0.0208,  0.0552, -0.0340, -0.0824, -0.2163,\n",
            "          -0.0973,  0.0726, -0.0750, -0.0855,  0.0375, -0.0743,  0.1042,\n",
            "           0.1357, -0.0114,  0.0903, -0.0102, -0.0404]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.837616443634033\n",
            "i: 49 char:  \n",
            "output: tensor([[[-0.0620, -0.0589,  0.0411,  0.0333, -0.0553, -0.1068, -0.1972,\n",
            "          -0.1241,  0.0768, -0.0468, -0.0946, -0.0084, -0.0822,  0.0729,\n",
            "           0.1523, -0.0304,  0.1103,  0.0115, -0.0452]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9347167015075684\n",
            "i: 50 char: y\n",
            "output: tensor([[[-0.0277, -0.1024,  0.0528,  0.0200, -0.0315, -0.0736, -0.1753,\n",
            "          -0.0794,  0.0681, -0.0310, -0.0986, -0.0256, -0.1031,  0.0827,\n",
            "           0.0945, -0.0202,  0.0945,  0.0255, -0.0543]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.9579579830169678\n",
            "i: 51 char: o\n",
            "output: tensor([[[-8.4380e-03, -7.1408e-02,  5.6047e-02,  3.7341e-02, -3.3542e-02,\n",
            "          -1.2525e-01, -2.3754e-01, -6.7388e-02,  5.4247e-02, -4.2931e-02,\n",
            "          -5.7531e-02, -6.0707e-05, -9.1772e-02,  8.1432e-02,  1.1014e-01,\n",
            "          -2.1976e-02,  1.1551e-01,  1.6595e-02, -2.5968e-02]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.8499860763549805\n",
            "i: 52 char: u\n",
            "output: tensor([[[-0.0375, -0.0414,  0.0014,  0.0444, -0.0501, -0.1165, -0.2213,\n",
            "          -0.0933,  0.0397, -0.0361, -0.0487,  0.0013, -0.0859,  0.0762,\n",
            "           0.1341, -0.0129,  0.1017,  0.0285, -0.0432]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "loss: 2.794593334197998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finalmente, con el token final, deberíamos esperar generar el token `<EOS>`, para que la RNN aprenda cuándo dejar de generar caracteres.**"
      ],
      "metadata": {
        "id": "mE03Pet2NLzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ingresar el target anterior y el estado olculto anterior al modelo\n",
        "output, hidden = model(target, hidden)\n",
        "\n",
        "# obtener el tensor en 2D del caracter <EOS>\n",
        "eos = vocab_stoi[\"<EOS>\"]\n",
        "target = torch.Tensor([eos]).long().unsqueeze(0)\n",
        "\n",
        "# redimensionar output a un tensor 2D, redimensionar el target a un tensor 1D y calcular la pérdida\n",
        "loss = loss_function(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "\n",
        "print(f\"char:{vocab_itos[eos]} output: {output} loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqULcpPBNaDh",
        "outputId": "edaae37f-900e-48dc-f328-6376d04d4dab"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char:<EOS> output: tensor([[[-0.0178, -0.0918,  0.0321,  0.0305, -0.0473, -0.0778, -0.2160,\n",
            "          -0.0814,  0.0509,  0.0039, -0.1065, -0.0217, -0.0715,  0.0601,\n",
            "           0.1044,  0.0148,  0.0725, -0.0019, -0.0377]]],\n",
            "       grad_fn=<AddBackward0>) loss: 2.9636733531951904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTA: En la práctica, realmente no necesitamos un bucle. Hay que recordar que en una RNN, el módulo `nn.RNN` puede tomar una secuencia completa como entrada.\n",
        "\n",
        "Podemos hacer lo mismo aquí:\n",
        "\n"
      ],
      "metadata": {
        "id": "JV7sfqtQQoRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definir el tweet de caracteres, poner al inicio y fin los tokens \"especiales\"\n",
        "tweet_char = [\"<BOS>\"] + list(tweet) + [\"<EOS>\"]\n",
        "\n",
        "# obtener los índices del tweet\n",
        "tweet_indexes = [vocab_stoi[ch] for ch in tweet_char]\n",
        "\n",
        "# convertir el tweet codificado a tensores en 2D\n",
        "tweet_tensor = torch.Tensor(tweet_indexes).long().unsqueeze(0)\n",
        "\n",
        "# visuaizar el shape del tensor\n",
        "print(f\"tweet_tensor.shape:\\t{tweet_tensor.shape}\\n\") # [1, 55] --> 53 (tweet) + 2 (tokens especiales)\n",
        "\n",
        "print (f\"tweet_tensor:\\t{tweet_tensor}\\n\")\n",
        "print (f'17:\\t{vocab_itos[17]}\\t18:\\t{vocab_itos[18]}\\n\\n')\n",
        "\n",
        "print (f\"tweet_tensor[:,:-1]:\\t{tweet_tensor[:,:-1]}\\n\\n\") # <EOS> nunca es un token de entrada\n",
        "print (f\"tweet_tensor[:,1:]:\\t{tweet_tensor[:,1:]}\\n\\n\") # <BOS> nunca es un token objetivo\n",
        "\n",
        "# ingresar el tweet al modelo\n",
        "output, hidden = model(tweet_tensor[:,:-1])\n",
        "\n",
        "# vector de targets\n",
        "target = tweet_tensor[:,1:]\n",
        "\n",
        "# redimensionar output a un tensor 2D, redimensionar el target a un tensor 1D y calcular la pérdida\n",
        "loss = loss_function(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "\n",
        "print (f\"pérdida:\\t{loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d3XjtqQ_x6",
        "outputId": "576ac778-0e2f-4560-bb80-7f39a2ca4e9a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweet_tensor.shape:\ttorch.Size([1, 55])\n",
            "\n",
            "tweet_tensor:\ttensor([[17,  9,  3,  7,  2,  2,  2, 11, 12, 13,  5,  0,  9, 11,  4,  7, 11,  7,\n",
            "         12,  0,  6, 11, 15,  0,  6,  7, 11,  5,  0,  9, 11,  8,  0,  9, 11, 12,\n",
            "         16,  3, 11,  1, 12, 16, 11, 10,  3, 11, 15,  4, 12, 16, 11,  9, 13, 14,\n",
            "         18]])\n",
            "\n",
            "17:\t<BOS>\t18:\t<EOS>\n",
            "\n",
            "\n",
            "tweet_tensor[:,:-1]:\ttensor([[17,  9,  3,  7,  2,  2,  2, 11, 12, 13,  5,  0,  9, 11,  4,  7, 11,  7,\n",
            "         12,  0,  6, 11, 15,  0,  6,  7, 11,  5,  0,  9, 11,  8,  0,  9, 11, 12,\n",
            "         16,  3, 11,  1, 12, 16, 11, 10,  3, 11, 15,  4, 12, 16, 11,  9, 13, 14]])\n",
            "\n",
            "\n",
            "tweet_tensor[:,1:]:\ttensor([[ 9,  3,  7,  2,  2,  2, 11, 12, 13,  5,  0,  9, 11,  4,  7, 11,  7, 12,\n",
            "          0,  6, 11, 15,  0,  6,  7, 11,  5,  0,  9, 11,  8,  0,  9, 11, 12, 16,\n",
            "          3, 11,  1, 12, 16, 11, 10,  3, 11, 15,  4, 12, 16, 11,  9, 13, 14, 18]])\n",
            "\n",
            "\n",
            "pérdida:\t2.955975294113159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La entrada del modelo de red neuronal es la secuencia completa de tokens de entrada del tweet (todo, desde `<BOS>` hasta el último carácter del tweet).\n",
        "\n",
        "La red neuronal genera una distribución de predicción del siguiente token en cada paso.\n",
        "\n",
        "Se puede comparar cada uno de estos con el objetivo real (**the ground-truth or actual target token**)."
      ],
      "metadata": {
        "id": "UUOE5xjQVBo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ciclo de entrenamiento (para aprender a generar el tweet único):\n",
        "\n",
        "# definir optimizador\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# definir la función de pérdida\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 500\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output, _ = model(tweet_tensor[:,:-1])\n",
        "    \n",
        "    # redimensionar output a un tensor 2D, redimensionar el target a un tensor 1D y calcular la pérdida\n",
        "    loss = loss_function(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "    \n",
        "    # actualizar los pesos\n",
        "    optimizer.step()\n",
        "\n",
        "    # imprimir el \"epoch\" y la pérdida\n",
        "    if (i+1) % 100 == 0:\n",
        "        print(f\"[Iteración {i+1}] Loss {float(loss)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyPRWfDcVvwg",
        "outputId": "c571aafa-ea77-47a8-f65d-3e280c2eb1c3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iteración 100] Loss 1.8953173160552979\n",
            "[Iteración 200] Loss 0.2369401603937149\n",
            "[Iteración 300] Loss 0.038695141673088074\n",
            "[Iteración 400] Loss 0.016332807019352913\n",
            "[Iteración 500] Loss 0.009432274848222733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La pérdida obtenida del entrenamiento está disminuyendo con el entrenamiento, que es lo que esperamos."
      ],
      "metadata": {
        "id": "VUU3UWseXF99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de un token\n",
        "En este punto, se desea ver si el modelo realmente está aprendiendo algo.\n",
        "\n",
        "Entonces, ¿Cómo se va a utilizar el modelo para generar texto?. \n",
        "\n",
        "Si podemos generar texto, podemos hacer una evaluación cualitativa de qué tan bien está funcionando la RNN.\n",
        "\n",
        "La principal diferencia entre el entrenamiento y el test (tiempo de generación) es que no tenemos los tokens verdaderos para alimentarlos como entradas a la RNN. En su lugar, se necesita muestrear un token en función de la distribución de predicción de la red neuronal.\n",
        "\n",
        "Pero, ¿cómo podemos probar un token de una distribución?\n",
        "\n",
        "Por un lado, siempre se puede tomar el token con mayor probabilidad (argmax). Esta ha sido la técnica de referencia en otras tareas de clasificación. \n",
        "\n",
        "Sin embargo, esta idea fallará aquí. La razón es que, en la práctica, **queremos poder generar una variedad de secuencias diferentes a partir del mismo modelo**. Una RNN que solamente pueda generar un nuevo tweet `happiness` es bastante inútil.\n",
        "\n",
        "En resumen, queremos algo de aleatoriedad. Se puede hacer usando los `logits` de salida del modelo para construir una distribución multinomial sobre los tokens y luego muestrear un token aleatorio de esa distribución multinomial.\n",
        "\n",
        "> **logits**: *son interpretados como predicciones (o resultados) no normalizadas (o aún no normalizadas) de un modelo.*\n",
        "\n",
        "Una distribución multinomial que se puede elegir es la distribución que se obtiene después de aplicar el la función `softmax` en las salidas. Sin embargo, haremos una cosa más: \n",
        "\n",
        "Se agrega un **parámetro de temperatura** para manipular las salidas de la función `softmax`.\n",
        "\n",
        "Se puede establecer una temperatura más alta para que la probabilidad de cada token sea más uniforme (más aleatoria), o una temperatura más baja para asignar más probabilidad a los tokens con un `logit` más alto (salida). \n",
        "\n",
        "Una temperatura más alta significa que obtendremos una muestra más diversa, con potencialmente más errores.\n",
        "\n",
        "Una temperatura más baja significa que podemos ver repeticiones de la misma secuencia de alta probabilidad.\n"
      ],
      "metadata": {
        "id": "G8pbDkPnXmJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_sequence(model, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "   \n",
        "    # entrada\n",
        "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long()\n",
        "\n",
        "    # capa de estados ocultos\n",
        "    hidden = None\n",
        "\n",
        "    # iterar para generar texto\n",
        "    for p in range(max_len):\n",
        "        # pasar parámetros al modelo. Agregar una dimension a inp --> tensor 2D\n",
        "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "\n",
        "        # obtener una muestra de la red como una distribución multinomial\n",
        "        # torch.div(input, other) --> divide cada elemento de la entrada \"input\" por el elemento correspondiente de \"other\".\n",
        "        # torch.exp(input) --> devuelve un nuevo tensor con la exponencial de los elementos del tensor de entrada \"input\".\n",
        "        # print (f\"output.shape:\\t{output.shape}\") # torch.Size([1, 1, 19])\n",
        "        # print (f\"output.data.view(-1):\\t{output.data.view(-1).shape}\") # torch.Size([19])\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "\n",
        "        # torch.multinomial(input, num_samples) --> devuelve un tensor donde cada fila contiene índices muestreados (\"num_samples\") de la distribución \n",
        "        # de probabilidad multinomial ubicada en la fila correspondiente del tensor \"input\".\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0]) # obtener el mejor\n",
        "\n",
        "        # agregar el carácter predicho a la cadena y utilizarlo como siguiente entrada\n",
        "        predicted_char = vocab_itos[top_i]\n",
        "        \n",
        "        # si se llega al final de la secuencia (tweet) predicha, salir del ciclo\n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "\n",
        "        # agregar el caracter predicho a la cadena generada  \n",
        "        generated_sequence += predicted_char\n",
        "\n",
        "        # convertir la siguiente entrada a un tensor long\n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        "\n",
        "print (tweet,'\\n')\n",
        "\n",
        "print(sample_sequence(model, temperature=0.5))\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=5.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP5Q3i5fbCa1",
        "outputId": "13cf42a4-fb54-4f91-811d-1378f4bc1b7a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes... today is star wars day may the 4th be with you \n",
            "\n",
            "yes... today is star wars day may the 4th be with you\n",
            "yes... today is star wars day may the 4th be with you\n",
            "yes... today is star wars day may the 4th be withdyou\n",
            "yes... today ss star raas day may the 4th 4e  ith you\n",
            "des..a thaywissstar w\n",
            " swet<BOS> o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que solo se ha entrenado el modelo en una sola secuencia, aún no se observa el efecto del parámetro de `temperatura`. \n",
        "\n",
        "Por ahora, el resultado de las llamadas a la función `sample_sequence` indica que el entrenamiento parece razonable."
      ],
      "metadata": {
        "id": "JDJe1tNNgWGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio\n",
        "Completar el entrenamiento con el conjunto de datos completo.\n",
        "\n",
        "\n",
        "*   Usar los tweets de la clase `happiness`, y\n",
        "*   el conjunto de tweets (100) en español de la clase pasada.\n",
        "*   O emplear el libro \"The Time Machine, by H. G. Wells [1898]\" del repositorio, o utilizar otro libro en español para generar texto con la GRU.  \n",
        "\n"
      ],
      "metadata": {
        "id": "8BODo1F5gprQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # EJEMPLO TOMADO DE https://stackoverflow.com/questions/66964723/get-the-length-of-every-sentence-before-padding-in-torchtext-bucketiterator\n",
        "\n",
        "# import torchtext.legacy.data as data\n",
        "\n",
        "\n",
        "# # sample data\n",
        "# text = [\n",
        "#    'This is sentence 1.',\n",
        "#    'This sentence is a bit longer than the previous sentence.'\n",
        "#  ]\n",
        "\n",
        "# # define field -- notice include_lengths is set to True\n",
        "# text_field = data.Field(include_lengths=True, tokenize=lambda x: x.split())  \n",
        "# fields = [('text', text_field)]\n",
        "\n",
        "# # create dataset and build vocabulary\n",
        "# examples = [data.Example.fromlist([t], fields) for t in text]\n",
        "# dataset = data.Dataset(examples, fields)\n",
        "# text_field.build_vocab(dataset)\n",
        "\n",
        "# print (dict(text_field.vocab.stoi))\n",
        "# print (len(dict(text_field.vocab.stoi)))\n",
        "# print (text_field.vocab.itos)\n",
        "\n",
        "# # create iterator\n",
        "# data_iter = data.BucketIterator(dataset, batch_size=2m, shuffle=False)\n",
        "\n",
        "# # the text field will now return both the data tensor and the length of the input text\n",
        "# for x in data_iter:\n",
        "#      print('Data:', x.text[0])\n",
        "#      print('Lengths:', x.text[1])"
      ],
      "metadata": {
        "id": "t-aUIDYldg2t"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext.legacy.data as data\n",
        "\n",
        "# definir Field\n",
        "# documentación de Field: https://torchtext.readthedocs.io/en/latest/data.html#field\n",
        "text_field = data.Field(sequential = True,                 # Default: True. Indica que se tokenizara\n",
        "                                  tokenize = lambda x: x,  # Default: string.split. Tokenizar a nivel caracter\n",
        "                                  include_lengths = True,  # Default: False. Para realizar un seguimiento de la longitud de las secuencias, para el procesamiento por lotes\n",
        "                                  batch_first = True,      # Default: False. Para producir tensores con la dimensión del lote al inicio.\n",
        "                                  use_vocab = True,        # Default: True. Para convertir cada carácter en un índice entero\n",
        "                                  init_token = \"<BOS>\",    # Default: None. Definir el token BOS\n",
        "                                  eos_token = \"<EOS>\",     # Default: None. Definir el token EOS\n",
        "                                  pad_token = None,        # Default: “<pad>”.\n",
        "                                  unk_token = None)        # Default: “<unk>”.\n",
        "\n",
        "fields = [('text', text_field)]\n",
        "\n",
        "print (type(tweets))\n",
        "print (f\"\\n{tweets[0:2]}\")\n",
        "\n",
        "# obtener las muestras\n",
        "# documentación de Example https://torchtext.readthedocs.io/en/latest/data.html#example\n",
        "muestras = [data.Example.fromlist([t], fields) for t in tweets]\n",
        "\n",
        "# construir el Dataset\n",
        "# documentación de Dataset: https://torchtext.readthedocs.io/en/latest/data.html#dataset\n",
        "dataset = data.Dataset(muestras, fields)\n",
        "\n",
        "# construir el vocabulario\n",
        "text_field.build_vocab(dataset)\n",
        "\n",
        "vocab_stoi = text_field.vocab.stoi # para que no tengamos que reescribir la función sample_sequence\n",
        "vocab_itos = text_field.vocab.itos # para que no tengamos que reescribir la función sample_sequence\n",
        "vocab_size = len(text_field.vocab.itos)\n",
        "\n",
        "print (f\"\\nvocab_stoi:\\t{dict(vocab_stoi)}\\n\")\n",
        "print (f\"vocab_itos:\\t{vocab_itos}\\n\")\n",
        "print (f\"vocab_size:\\t{vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxoaTfIngiqK",
        "outputId": "e62bd220-57fa-49f6-ccb9-fb21cddabf30"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "\n",
            "[\"mmm much better day... so far it's still quite early. last day of uds\", 'so great to see oin amp; cynthia. so happy. dinner was great, cute little place. too bad oin got sick afterwards.']\n",
            "\n",
            "vocab_stoi:\t{'<BOS>': 0, '<EOS>': 1, ' ': 2, 'e': 3, 'o': 4, 't': 5, 'a': 6, 'i': 7, 'n': 8, 's': 9, 'h': 10, 'r': 11, 'l': 12, 'd': 13, 'y': 14, 'm': 15, 'u': 16, 'g': 17, 'w': 18, '.': 19, 'c': 20, 'f': 21, 'p': 22, 'b': 23, 'k': 24, 'v': 25, \"'\": 26, ',': 27, 'j': 28, 'x': 29, ';': 30, 'q': 31, 'z': 32, '0': 33, '1': 34, '2': 35, '3': 36, ':': 37, '4': 38, '5': 39, '8': 40, '6': 41, '9': 42, '7': 43}\n",
            "\n",
            "vocab_itos:\t['<BOS>', '<EOS>', ' ', 'e', 'o', 't', 'a', 'i', 'n', 's', 'h', 'r', 'l', 'd', 'y', 'm', 'u', 'g', 'w', '.', 'c', 'f', 'p', 'b', 'k', 'v', \"'\", ',', 'j', 'x', ';', 'q', 'z', '0', '1', '2', '3', ':', '4', '5', '8', '6', '9', '7']\n",
            "\n",
            "vocab_size:\t44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar funcion para el entrenamiento de todo el conjunto de datos"
      ],
      "metadata": {
        "id": "suF1EWdWkALj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_iter, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        avg_loss = 0\n",
        "        for x in data_iter:\n",
        "            target = x.text[0][:, 1:]\n",
        "            inp = x.text[0][:, :-1]\n",
        "\n",
        "            # resetear, limpiar pesos\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # forward\n",
        "            output, _ = model(inp)\n",
        "\n",
        "            # calcular la pérdida\n",
        "            loss = loss_function(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "\n",
        "            # actualizar pesos\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            it += 1 # incrementar el contador de la iteración\n",
        "            if it % print_every == 0:\n",
        "                print(f\"[EPOCH {e+1}/{num_epochs} - iteración: {it:,}] Loss {float(avg_loss/print_every)}\")\n",
        "                # print(f\"\\t{sample_sequence(model, max_len=140, temperature=0.8)}\") # imprimir texto generado \n",
        "                avg_loss = 0"
      ],
      "metadata": {
        "id": "xxO9GAvgkGI3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# realizar el entrenamiento\n",
        "model = TextGenerator(vocab_size, 64)\n",
        "data_iter = data.BucketIterator(dataset, batch_size=1, shuffle=False)\n",
        "train(model, data_iter, batch_size=1, num_epochs=1, lr=0.004, print_every=100)\n",
        "# train(model, data_iter, batch_size=32, num_epochs=2, lr=0.004, print_every=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9EGipqxkOVn",
        "outputId": "16493451-92ee-4fe0-ce8d-2eda6704bdfe"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH 1/1 - iteración: 100] Loss 3.048532724380493\n",
            "[EPOCH 1/1 - iteración: 200] Loss 2.6520895957946777\n",
            "[EPOCH 1/1 - iteración: 300] Loss 2.4878878593444824\n",
            "[EPOCH 1/1 - iteración: 400] Loss 2.442838191986084\n",
            "[EPOCH 1/1 - iteración: 500] Loss 2.377424716949463\n",
            "[EPOCH 1/1 - iteración: 600] Loss 2.3510961532592773\n",
            "[EPOCH 1/1 - iteración: 700] Loss 2.2481038570404053\n",
            "[EPOCH 1/1 - iteración: 800] Loss 2.2275185585021973\n",
            "[EPOCH 1/1 - iteración: 900] Loss 2.2008490562438965\n",
            "[EPOCH 1/1 - iteración: 1,000] Loss 2.2274558544158936\n",
            "[EPOCH 1/1 - iteración: 1,100] Loss 2.131369113922119\n",
            "[EPOCH 1/1 - iteración: 1,200] Loss 2.1761457920074463\n",
            "[EPOCH 1/1 - iteración: 1,300] Loss 2.153590440750122\n",
            "[EPOCH 1/1 - iteración: 1,400] Loss 2.0738468170166016\n",
            "[EPOCH 1/1 - iteración: 1,500] Loss 2.12138295173645\n",
            "[EPOCH 1/1 - iteración: 1,600] Loss 2.0587167739868164\n",
            "[EPOCH 1/1 - iteración: 1,700] Loss 2.034724712371826\n",
            "[EPOCH 1/1 - iteración: 1,800] Loss 2.0238351821899414\n",
            "[EPOCH 1/1 - iteración: 1,900] Loss 2.0665836334228516\n",
            "[EPOCH 1/1 - iteración: 2,000] Loss 1.9934945106506348\n",
            "[EPOCH 1/1 - iteración: 2,100] Loss 2.0000193119049072\n",
            "[EPOCH 1/1 - iteración: 2,200] Loss 2.0086464881896973\n",
            "[EPOCH 1/1 - iteración: 2,300] Loss 1.9726147651672363\n",
            "[EPOCH 1/1 - iteración: 2,400] Loss 1.975204348564148\n",
            "[EPOCH 1/1 - iteración: 2,500] Loss 1.940536618232727\n",
            "[EPOCH 1/1 - iteración: 2,600] Loss 1.9278054237365723\n",
            "[EPOCH 1/1 - iteración: 2,700] Loss 1.9805898666381836\n",
            "[EPOCH 1/1 - iteración: 2,800] Loss 2.0257561206817627\n",
            "[EPOCH 1/1 - iteración: 2,900] Loss 1.930893898010254\n",
            "[EPOCH 1/1 - iteración: 3,000] Loss 1.9964560270309448\n",
            "[EPOCH 1/1 - iteración: 3,100] Loss 1.9785141944885254\n",
            "[EPOCH 1/1 - iteración: 3,200] Loss 1.9380967617034912\n",
            "[EPOCH 1/1 - iteración: 3,300] Loss 1.941474437713623\n",
            "[EPOCH 1/1 - iteración: 3,400] Loss 1.932566523551941\n",
            "[EPOCH 1/1 - iteración: 3,500] Loss 1.8688429594039917\n",
            "[EPOCH 1/1 - iteración: 3,600] Loss 1.8812229633331299\n",
            "[EPOCH 1/1 - iteración: 3,700] Loss 1.8932178020477295\n",
            "[EPOCH 1/1 - iteración: 3,800] Loss 1.8230165243148804\n",
            "[EPOCH 1/1 - iteración: 3,900] Loss 1.901076078414917\n",
            "[EPOCH 1/1 - iteración: 4,000] Loss 1.896165370941162\n",
            "[EPOCH 1/1 - iteración: 4,100] Loss 1.8492792844772339\n",
            "[EPOCH 1/1 - iteración: 4,200] Loss 1.8329718112945557\n",
            "[EPOCH 1/1 - iteración: 4,300] Loss 1.9171313047409058\n",
            "[EPOCH 1/1 - iteración: 4,400] Loss 1.8892742395401\n",
            "[EPOCH 1/1 - iteración: 4,500] Loss 1.8408411741256714\n",
            "[EPOCH 1/1 - iteración: 4,600] Loss 1.9073066711425781\n",
            "[EPOCH 1/1 - iteración: 4,700] Loss 1.9310126304626465\n",
            "[EPOCH 1/1 - iteración: 4,800] Loss 1.8719538450241089\n",
            "[EPOCH 1/1 - iteración: 4,900] Loss 1.8051409721374512\n",
            "[EPOCH 1/1 - iteración: 5,000] Loss 1.9285645484924316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generar texto a partir del modelo entrenado con GRU"
      ],
      "metadata": {
        "id": "NbRUWOHPkhwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(sample_sequence(model, temperature=0.3))\n",
        "print(sample_sequence(model, temperature=0.5))\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "# print(sample_sequence(model, temperature=0.8))\n",
        "# print(sample_sequence(model, temperature=1.0))\n",
        "# print(sample_sequence(model, temperature=1.0))\n",
        "# print(sample_sequence(model, temperature=1.5))\n",
        "# print(sample_sequence(model, temperature=1.5))\n",
        "# print(sample_sequence(model, temperature=2.0))\n",
        "# print(sample_sequence(model, temperature=2.0))\n",
        "# print(sample_sequence(model, temperature=5.0))\n",
        "# print(sample_sequence(model, temperature=5.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvHe-GS4kfeW",
        "outputId": "42499f01-4180-4e7b-bdba-7708a5ecc88d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy mother's day to see to make a good more the sunday\n",
            "happy mother's day mome happy mother's day to her your thanks for make to see well for the chee in i\n",
            "forwere we with vishome in that day\n"
          ]
        }
      ]
    }
  ]
}