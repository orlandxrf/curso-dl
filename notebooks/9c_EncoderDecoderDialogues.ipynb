{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9c_EncoderDecoderDialogues.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orlandxrf/curso-dl/blob/main/notebooks/9c_EncoderDecoderDialogues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cQWwAo1uqA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05ad2cb-e393-42f2-b120-7bc515a90e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy import data\n",
        "import spacy\n",
        "\n",
        "# Helper libraries\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjunto de datos\n",
        "El conjunto de datos original se pude descargar en [Cornell Movie - Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)"
      ],
      "metadata": {
        "id": "d2eb_oFm4UHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# establecer parametros para obtener el conjunto de datos del repositorio de Github\n",
        "import os\n",
        "\n",
        "data_folder = 'data'\n",
        "URL1 = 'https://raw.githubusercontent.com/orlandxrf/curso-dl/main/data/movie_lines.txt'\n",
        "URL2 = 'https://raw.githubusercontent.com/orlandxrf/curso-dl/main/data/movie_conversations.txt'\n",
        "filepath1 = os.path.join(data_folder, 'movie_lines.txt')\n",
        "filepath2 = os.path.join(data_folder, 'movie_conversations.txt')\n",
        "\n",
        "# crear carpeta para almacenar el conjunto de datos\n",
        "! mkdir {data_folder}\n",
        "\n",
        "# descargar conjunto de datos y alamcenar\n",
        "! wget -nc {URL1} -O {filepath1}\n",
        "! wget -nc {URL2} -O {filepath2}\n",
        "\n",
        "# comprobrar\n",
        "! ls -lh data/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv-OACuk2yCy",
        "outputId": "61dc2323-ff3a-4aa5-f1a9-7f6515663b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "File ‘data/movie_lines.txt’ already there; not retrieving.\n",
            "File ‘data/movie_conversations.txt’ already there; not retrieving.\n",
            "-rw-r--r-- 1 root root 6.5M Mar 30 20:40 data/movie_conversations.txt\n",
            "-rw-r--r-- 1 root root  34M Mar 30 20:40 data/movie_lines.txt\n",
            "-rw-r--r-- 1 root root  24M Mar 30 20:51 data/movies_dialogues.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! head 10 data/movie_lines.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUbK6Xxm7e-U",
        "outputId": "5913855e-1a11-40ec-cdf0-65febf9645fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open '10' for reading: No such file or directory\n",
            "==> data/movie_lines.txt <==\n",
            "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
            "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
            "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
            "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
            "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
            "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
            "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
            "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n",
            "L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
            "L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! head 10 data/movie_conversations.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNkYNlL27jki",
        "outputId": "d803301b-19a9-4ce8-de63-550dd6e27e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open '10' for reading: No such file or directory\n",
            "==> data/movie_conversations.txt <==\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def readTxtFile(path):\n",
        "    data = []\n",
        "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.replace('\\n', '')\n",
        "            data.append(line)\n",
        "    f.close()\n",
        "    return data\n",
        "\n",
        "mv_lines = readTxtFile('data/movie_lines.txt')\n",
        "mv_conversations = readTxtFile('data/movie_conversations.txt')\n"
      ],
      "metadata": {
        "id": "wcfGiyvb4hma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtener todas las líneas de películas con su respectivo ID\n",
        "\n",
        "# id2line = {line.split(' +++$+++ ')[0] : line.split(' +++$+++ ')[4] for line in mv_lines if len(line.split(' +++$+++ '))==5}\n",
        "\n",
        "id2line = {}\n",
        "count = 0\n",
        "\n",
        "for line in mv_lines:\n",
        "    line = line.split(' +++$+++ ')\n",
        "    if len(line) == 5:\n",
        "        if \"\\t\" in line[4]: id2line[line[0]] = line[4].replace('\\t', ' ')\n",
        "        else: id2line[line[0]] = line[4]\n",
        "\n",
        "for i, x in enumerate(id2line):\n",
        "    if i==10: break\n",
        "    print (f\"{i+1}\\t{x}\\t{id2line[x]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxrnQJ1Y71B3",
        "outputId": "70fd56fb-bf21-4dd4-981b-fd6f245cc4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\tL1045\tThey do not!\n",
            "2\tL1044\tThey do to!\n",
            "3\tL985\tI hope so.\n",
            "4\tL984\tShe okay?\n",
            "5\tL925\tLet's go.\n",
            "6\tL924\tWow\n",
            "7\tL872\tOkay -- you're gonna need to learn how to lie.\n",
            "8\tL871\tNo\n",
            "9\tL870\tI'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
            "10\tL869\tLike my fear of wearing pastels?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crear una lista de todos los ID de las conversaciones\n",
        "conversations = [eval(line.split(' +++$+++ ')[-1]) for line in mv_conversations]\n",
        "\n",
        "for i, x in enumerate(conversations):\n",
        "    if i==10: break\n",
        "    print (f\"{i+1}\\t{x}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtFTGj2_-Szh",
        "outputId": "a189206d-4af1-46d0-adb2-09efc1af6ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t['L194', 'L195', 'L196', 'L197']\n",
            "2\t['L198', 'L199']\n",
            "3\t['L200', 'L201', 'L202', 'L203']\n",
            "4\t['L204', 'L205', 'L206']\n",
            "5\t['L207', 'L208']\n",
            "6\t['L271', 'L272', 'L273', 'L274', 'L275']\n",
            "7\t['L276', 'L277']\n",
            "8\t['L280', 'L281']\n",
            "9\t['L363', 'L364']\n",
            "10\t['L365', 'L366']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mostrar conversaciones por su ID\n",
        "\n",
        "for x in conversations[5]:\n",
        "    print (f\"{x}\\t{id2line[x]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nwe42r2AzTT",
        "outputId": "d7fc083b-d9a8-4cb1-a1da-120dff680194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L271\tC'esc ma tete. This is my head\n",
            "L272\tRight.  See?  You're ready for the quiz.\n",
            "L273\tI don't want to know how to say that though.  I want to know useful things. Like where the good stores are.  How much does champagne cost?  Stuff like Chat.  I have never in my life had to point out my head to someone.\n",
            "L274\tThat's because it's such a nice one.\n",
            "L275\tForget French.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ordenar las oraciones en preguntas (inputs) y respuestas (targets)\n",
        "questions, answers = [], []\n",
        "\n",
        "for conv in conversations:\n",
        "    for i in range(len(conv)-1):\n",
        "        questions.append(id2line[conv[i]])\n",
        "        answers.append(id2line[conv[i+1]])\n",
        "\n",
        "print (f\"Longitud de preguntas {len(questions):,}\")\n",
        "print (f\"Longitud de respuestas {len(answers):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEPT9rARBmRq",
        "outputId": "c2a857a7-4164-4c2b-8ca1-7742fa95e0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud de preguntas 221,616\n",
            "Longitud de respuestas 221,616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ver los pares de conversación\n",
        "for i in range(10):\n",
        "    print (f\"QUESTION [{i+1}]:\\t{questions[i]}\")\n",
        "    print (f\"ANSWERS [{i+1}]:\\t{answers[i]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ienlq6f_BmS8",
        "outputId": "d6b88d74-4145-4952-b90d-65c32a611a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION [1]:\tCan we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
            "ANSWERS [1]:\tWell, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "QUESTION [2]:\tWell, I thought we'd start with pronunciation, if that's okay with you.\n",
            "ANSWERS [2]:\tNot the hacking and gagging and spitting part.  Please.\n",
            "\n",
            "QUESTION [3]:\tNot the hacking and gagging and spitting part.  Please.\n",
            "ANSWERS [3]:\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "\n",
            "QUESTION [4]:\tYou're asking me out.  That's so cute. What's your name again?\n",
            "ANSWERS [4]:\tForget it.\n",
            "\n",
            "QUESTION [5]:\tNo, no, it's my fault -- we didn't have a proper introduction ---\n",
            "ANSWERS [5]:\tCameron.\n",
            "\n",
            "QUESTION [6]:\tCameron.\n",
            "ANSWERS [6]:\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
            "\n",
            "QUESTION [7]:\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
            "ANSWERS [7]:\tSeems like she could get a date easy enough...\n",
            "\n",
            "QUESTION [8]:\tWhy?\n",
            "ANSWERS [8]:\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
            "\n",
            "QUESTION [9]:\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
            "ANSWERS [9]:\tThat's a shame.\n",
            "\n",
            "QUESTION [10]:\tGosh, if only we could find Kat a boyfriend...\n",
            "ANSWERS [10]:\tLet me see what I can do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# función para re-escribir expresiones y limpiar el texto\n",
        "def clean_text(text):\n",
        "    import re\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "FeKGamFTHUaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# limpiar las preguntas y respuestas\n",
        "\n",
        "# clean_questions = [clean_text(question) for question in questions]\n",
        "# clean_answers = [clean_text(answer) for answer in answers]\n",
        "\n",
        "clean_questions, clean_answers = [], []\n",
        "\n",
        "print (\"Limpiando preguntas\")\n",
        "tmp_q_indexes = []\n",
        "for i, question in enumerate(questions):\n",
        "    tmp_clean = clean_text(question)\n",
        "    tmp_clean = tmp_clean.strip()\n",
        "    if len(tmp_clean) <= 0: # no hay pregunta, esta vacía\n",
        "        tmp_q_indexes.append(i)\n",
        "    else:\n",
        "        clean_questions.append(tmp_clean)\n",
        "\n",
        "print (f\"Preguntas vacías: {len(tmp_q_indexes):,}\\n\")\n",
        "\n",
        "print (\"Limpiando respuestas\")\n",
        "tmp_a_indexes = []\n",
        "for i, answer in enumerate(answers):\n",
        "    if i in tmp_q_indexes: continue # no seguir, la pregunta esta vacía\n",
        "    tmp_clean = clean_text(answer)\n",
        "    tmp_clean = tmp_clean.strip()\n",
        "    if len(tmp_clean) <= 0: # si no hay respuesta\n",
        "        tmp_a_indexes.append(i)\n",
        "        del clean_questions[i] # eliminar preguntas, no hay respuesta para completar el par\n",
        "    else:\n",
        "        clean_answers.append(tmp_clean)\n",
        "\n",
        "print (f\"Respuestas vacías: {len(tmp_a_indexes):,}\\n\")\n",
        "\n",
        "print (f\"Longitud de preguntas:\\t{len(clean_questions):,}\")\n",
        "print (f\"Longitud de respuestas:\\t{len(clean_answers):,}\\n\")\n",
        "\n",
        "# mostrar preguntas y respuestas limpias\n",
        "for i in range(10):\n",
        "    print (f\"QUESTION [{i+1}]:\\t{clean_questions[i]}\")\n",
        "    print (f\"ANSWERS [{i+1}]:\\t{clean_answers[i]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akobh29eJxPM",
        "outputId": "14d4830c-5fe6-44c5-a61d-81ff06dacac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limpiando preguntas\n",
            "Preguntas vacías: 197\n",
            "\n",
            "Limpiando respuestas\n",
            "Respuestas vacías: 137\n",
            "\n",
            "Longitud de preguntas:\t221,282\n",
            "Longitud de respuestas:\t221,282\n",
            "\n",
            "QUESTION [1]:\tcan we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad. again.\n",
            "ANSWERS [1]:\twell, i thought we would start with pronunciation, if that is okay with you.\n",
            "\n",
            "QUESTION [2]:\twell, i thought we would start with pronunciation, if that is okay with you.\n",
            "ANSWERS [2]:\tnot the hacking and gagging and spitting part. please.\n",
            "\n",
            "QUESTION [3]:\tnot the hacking and gagging and spitting part. please.\n",
            "ANSWERS [3]:\tokay... then how about we try out some french cuisine. saturday? night?\n",
            "\n",
            "QUESTION [4]:\tyou are asking me out. that is so cute. that is your name again?\n",
            "ANSWERS [4]:\tforget it.\n",
            "\n",
            "QUESTION [5]:\tno, no, it is my fault we did not have a proper introduction\n",
            "ANSWERS [5]:\tcameron.\n",
            "\n",
            "QUESTION [6]:\tcameron.\n",
            "ANSWERS [6]:\tthe thing is, cameron i am at the mercy of a particularly hideous breed of loser. my sister. i cannot date until she does.\n",
            "\n",
            "QUESTION [7]:\tthe thing is, cameron i am at the mercy of a particularly hideous breed of loser. my sister. i cannot date until she does.\n",
            "ANSWERS [7]:\tseems like she could get a date easy enough...\n",
            "\n",
            "QUESTION [8]:\twhy?\n",
            "ANSWERS [8]:\tunsolved mystery. she used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
            "\n",
            "QUESTION [9]:\tunsolved mystery. she used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
            "ANSWERS [9]:\tthat is a shame.\n",
            "\n",
            "QUESTION [10]:\tgosh, if only we could find kat a boyfriend...\n",
            "ANSWERS [10]:\tlet me see what i can do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparar el conjunto de datos"
      ],
      "metadata": {
        "id": "FYnT__O-LTqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def saveDataIntoFile(output_path, data, mode='a'):\n",
        "    g = open(output_path, mode)\n",
        "    g.write(data)\n",
        "    g.close()\n",
        "\n",
        "output_path = 'data/movies_dialogues.tsv'\n",
        "saveDataIntoFile(output_path, '', 'w') # crear/resetear archivo\n",
        "\n",
        "for i in range(len(clean_questions)):\n",
        "    tmp_data = \"{}\\t{}\\n\".format(clean_questions[i], clean_answers[i])\n",
        "    if len(tmp_data.split('\\t')) != 2:\n",
        "        print (tmp_data.split('\\t'))\n",
        "        print (i, clean_questions[i])\n",
        "        print (i, clean_answers[i])\n",
        "        break\n",
        "    saveDataIntoFile(output_path, tmp_data)\n",
        "\n",
        "print (f\"dataset creado en {output_path}\")\n",
        "\n",
        "! ls -lh data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o22td0TELX6G",
        "outputId": "db79b62a-1515-4f48-8fef-6a10c22f2f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset creado en data/movies_dialogues.tsv\n",
            "total 63M\n",
            "-rw-r--r-- 1 root root 6.5M Mar 30 20:40 movie_conversations.txt\n",
            "-rw-r--r-- 1 root root  34M Mar 30 20:40 movie_lines.txt\n",
            "-rw-r--r-- 1 root root  24M Mar 30 20:59 movies_dialogues.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 data/movies_dialogues.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IwCUgh0M3uM",
        "outputId": "2f313e0a-0958-44d5-925a-69693c7ba197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad. again.\twell, i thought we would start with pronunciation, if that is okay with you.\n",
            "well, i thought we would start with pronunciation, if that is okay with you.\tnot the hacking and gagging and spitting part. please.\n",
            "not the hacking and gagging and spitting part. please.\tokay... then how about we try out some french cuisine. saturday? night?\n",
            "you are asking me out. that is so cute. that is your name again?\tforget it.\n",
            "no, no, it is my fault we did not have a proper introduction\tcameron.\n",
            "cameron.\tthe thing is, cameron i am at the mercy of a particularly hideous breed of loser. my sister. i cannot date until she does.\n",
            "the thing is, cameron i am at the mercy of a particularly hideous breed of loser. my sister. i cannot date until she does.\tseems like she could get a date easy enough...\n",
            "why?\tunsolved mystery. she used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
            "unsolved mystery. she used to be really popular when she started high school, then it was just like she got sick of it or something.\tthat is a shame.\n",
            "gosh, if only we could find kat a boyfriend...\tlet me see what i can do.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('data/movies_dialogues.tsv', encoding='utf-8', errors='ignore') as f:\n",
        "#     for line in f:\n",
        "#         line = line.replace('\\n', '').split('\\t')\n",
        "#         if len(line[0].strip()) <= 0:\n",
        "#             print (line)\n",
        "#             # break\n",
        "#         if len(line[1].strip()) <= 0:\n",
        "#             print (line)\n",
        "#             # break\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "iNbRFWvjcfYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar el modelo en inglés para la tokenización\n",
        "spacy_english = spacy.load(\"en\")\n",
        "\n",
        "def tokenize_english(text):\n",
        "    return [token.text for token in spacy_english.tokenizer(text)]"
      ],
      "metadata": {
        "id": "fyP0wRMSPwuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# eliminar de memoria\n",
        "del questions\n",
        "del answers\n",
        "del clean_questions\n",
        "del clean_answers\n",
        "# ------------------------------------------------------\n",
        "\n",
        "MAX_VOCAB_SIZE = 30000\n",
        "MIN_COUNT = 3 # 1\n",
        "MAX_SEQUENCE_LENGTH = 10 # 15 # 20\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "# crear un objeto Field\n",
        "TEXT = data.Field(\n",
        "    tokenize = tokenize_english,\n",
        "    lower = True, \n",
        "    include_lengths = True, \n",
        "    init_token = '<sos>', \n",
        "    eos_token = '<eos>'\n",
        ")\n",
        "\n",
        "# especificar los objetos Field\n",
        "fields = [('input_sequence', TEXT), ('output_sequence', TEXT)]\n",
        "\n",
        "# establecer la ruta del conjunto de datos\n",
        "data_file = 'data/movies_dialogues.tsv'\n",
        "\n",
        "# construir el conjunto de datos con TabularDataset\n",
        "dialogue_data = data.TabularDataset(\n",
        "    path = data_file,\n",
        "    format = 'tsv',\n",
        "    fields = fields\n",
        ")\n",
        "\n",
        "print ( dialogue_data.fields)\n",
        "\n",
        "\n",
        "# construir el vocabulario, e incluir vectores pre-entrenados de GLoVe\n",
        "TEXT.build_vocab(\n",
        "    dialogue_data,\n",
        "    max_size = MAX_VOCAB_SIZE,\n",
        "    min_freq = MIN_COUNT,\n",
        "    # vectors = 'glove.6B.300d',\n",
        "    # unk_init = torch.Tensor.normal_\n",
        ")\n",
        "\n",
        "# dividir el dataset de diálogos en entrenamiento, validación y pruebas\n",
        "train_data, test_data = dialogue_data.split()\n",
        "train_data, valid_data = train_data.split()\n",
        "\n",
        "# crear iteradores de cada dataset\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x:len(x.input_sequence),\n",
        "    device = device\n",
        ")"
      ],
      "metadata": {
        "id": "l1ezKTREvDmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865c38ee-d899-4362-f220-0aafc6f5f193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_sequence': <torchtext.legacy.data.field.Field object at 0x7febf65a81d0>, 'output_sequence': <torchtext.legacy.data.field.Field object at 0x7febf65a81d0>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_english = None # liberar espacio de memoria\n",
        "\n",
        "print (f\"Muestras en el train:\\t {len(train_iterator.dataset):,}\")\n",
        "print (f\"Muestras en el valid:\\t {len(valid_iterator.dataset):,}\")\n",
        "print (f\"Muestras en el test:\\t {len(test_iterator.dataset):,}\\n\")\n",
        "\n",
        "print (f\"Batches en el train:\\t {len(train_iterator):,}\")\n",
        "print (f\"Batches en el valid:\\t {len(valid_iterator):,}\")\n",
        "print (f\"Batches en el test:\\t {len(test_iterator):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWBiP6sNdTmI",
        "outputId": "a8ff7d10-55b0-46b5-a1d0-1d8fd3b21017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Muestras en el train:\t 108,428\n",
            "Muestras en el valid:\t 46,469\n",
            "Muestras en el test:\t 66,385\n",
            "\n",
            "Batches en el train:\t 848\n",
            "Batches en el valid:\t 364\n",
            "Batches en el test:\t 519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir el modelo"
      ],
      "metadata": {
        "id": "uKK9xrbdoCYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "ze6dUjYUoEba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  \n",
        "    def __init__(self, hidden_size, embedding_size, embedding, num_layers=2, dropout=0.0):\n",
        "      \n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # Basic network params\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Embedding layer that will be shared with Decoder\n",
        "        self.embedding = embedding\n",
        "        \n",
        "        # Bidirectional GRU\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size,\n",
        "                          num_layers=num_layers,\n",
        "                          dropout=dropout,\n",
        "                          bidirectional=True)\n",
        "        \n",
        "    def forward(self, input_sequence, input_lengths):\n",
        "        \n",
        "        # Convert input_sequence to word embeddings\n",
        "        word_embeddings = self.embedding(input_sequence)\n",
        "        \n",
        "        \n",
        "        # Pack the sequence of embeddings\n",
        "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(word_embeddings, input_lengths)\n",
        "        \n",
        "        # Run the packed embeddings through the GRU, and then unpack the sequences\n",
        "        outputs, hidden = self.gru(packed_embeddings)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        \n",
        "        \n",
        "        # The ouput of a GRU has shape (seq_len, batch, hidden_size * num_directions)\n",
        "        # Because the Encoder is bidirectional, combine the results from the \n",
        "        # forward and reversed sequence by simply adding them together.\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "\n",
        "        return outputs, hidden\n"
      ],
      "metadata": {
        "id": "17urrw1HbTfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mecanismo de Atención"
      ],
      "metadata": {
        "id": "VqCYVCCOoh9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "      \n",
        "    def dot_score(self, hidden_state, encoder_states):\n",
        "        return torch.sum(hidden_state * encoder_states, dim=2)\n",
        "    \n",
        "            \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "       \n",
        "        attn_scores = self.dot_score(hidden, encoder_outputs)\n",
        "        \n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_scores = attn_scores.t()\n",
        "\n",
        "        # Apply mask so network does not attend <pad> tokens        \n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        # Return softmax over attention scores      \n",
        "        return F.softmax(attn_scores, dim=1).unsqueeze(1)"
      ],
      "metadata": {
        "id": "BWhAv1Y_oldq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "iylqgO53oZvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        \n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # Basic network params\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "                \n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers, \n",
        "                          dropout=dropout)\n",
        "        \n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attention(hidden_size)\n",
        "        \n",
        "    def forward(self, current_token, hidden_state, encoder_outputs, mask):\n",
        "      \n",
        "        # convert current_token to word_embedding\n",
        "        # embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding(current_token)\n",
        "        \n",
        "        # Pass through GRU\n",
        "        rnn_output, hidden_state = self.gru(embedded, hidden_state)\n",
        "        \n",
        "        # Calculate attention weights\n",
        "        attention_weights = self.attn(rnn_output, encoder_outputs, mask)\n",
        "        \n",
        "        # Calculate context vector\n",
        "        context = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        \n",
        "        # Concatenate  context vector and GRU output\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        \n",
        "        # Pass concat_output to final output layer\n",
        "        output = self.out(concat_output)\n",
        "        \n",
        "        # Return output and final hidden state\n",
        "        return output, hidden_state"
      ],
      "metadata": {
        "id": "wUCeph8WoY86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seq(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, vocab_size, \n",
        "                 device, pad_idx, eos_idx, sos_idx, teacher_forcing_ratio=0.5):\n",
        "        super(seq2seq, self).__init__()\n",
        "        \n",
        "        # Embedding layer shared by encoder and decoder\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        \n",
        "        # Encoder network\n",
        "        self.encoder = Encoder(hidden_size, \n",
        "                               embedding_size, \n",
        "                               self.embedding,\n",
        "                              num_layers=2,\n",
        "                              dropout=0.5)\n",
        "        \n",
        "        # Decoder network        \n",
        "        self.decoder = Decoder(self.embedding,\n",
        "                               embedding_size,\n",
        "                              hidden_size,\n",
        "                              vocab_size,\n",
        "                              n_layers=2,\n",
        "                              dropout=0.5)\n",
        "        \n",
        "        \n",
        "        # Indices of special tokens and hardware device \n",
        "        self.pad_idx = pad_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.sos_idx = sos_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, input_sequence):\n",
        "        return (input_sequence != self.pad_idx).permute(1, 0)\n",
        "        \n",
        "        \n",
        "    def forward(self, input_sequence, output_sequence, teacher_forcing_ratio=0.5):\n",
        "      \n",
        "        # Unpack input_sequence tuple\n",
        "        input_tokens = input_sequence[0].to(self.device)\n",
        "        input_lengths = input_sequence[1].to(self.device)\n",
        "      \n",
        "        # Unpack output_tokens, or create an empty tensor for text generation\n",
        "        if output_sequence is None:\n",
        "            inference = True\n",
        "            output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_idx).to(self.device)\n",
        "        else:\n",
        "            inference = False\n",
        "            output_tokens = output_sequence[0]\n",
        "            vocab_size = self.decoder.output_size\n",
        "        \n",
        "        batch_size = len(input_lengths)\n",
        "        max_seq_len = len(output_tokens)\n",
        "        \n",
        "        \n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)\n",
        "        \n",
        "        \n",
        "        # Pass through the first half of the network\n",
        "        encoder_outputs, hidden = self.encoder(input_tokens, input_lengths)\n",
        "        \n",
        "        # Ensure dim of hidden_state can be fed into Decoder\n",
        "        hidden =  hidden[:self.decoder.n_layers]\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        output = output_tokens[0,:]\n",
        "        \n",
        "        # Create mask\n",
        "        mask = self.create_mask(input_tokens)\n",
        "        \n",
        "        \n",
        "        # Step through the length of the output sequence one token at a time\n",
        "        # Teacher forcing is used to assist training\n",
        "        for t in range(1, max_seq_len):\n",
        "            output = output.unsqueeze(0)\n",
        "            \n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (output_tokens[t] if teacher_force else top1)\n",
        "            \n",
        "            # If we're in inference mode, keep generating until we produce an\n",
        "            # <eos> token\n",
        "            if inference and output.item() == self.eos_idx:\n",
        "                return outputs[:t]\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "qaTo8W0SsoGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenar el modelo"
      ],
      "metadata": {
        "id": "mrWUXfvpslRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funcion para medir el tiempo de entrenamiento\n",
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "xvi6-S2wm3SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_idx = TEXT.vocab.stoi['<pad>']\n",
        "eos_idx = TEXT.vocab.stoi['<eos>']\n",
        "sos_idx = TEXT.vocab.stoi['<sos>']\n",
        "# Size of embedding_dim should match the dim of pre-trained word embeddings!\n",
        "embedding_dim = 50\n",
        "hidden_dim = 512\n",
        "vocab_size = len(TEXT.vocab)\n",
        "model = seq2seq(embedding_dim,\n",
        "                 hidden_dim, \n",
        "                 vocab_size, \n",
        "                 device, pad_idx, eos_idx, sos_idx).to(device)\n",
        "# pretrained_embeddings = TEXT.vocab.vectors\n",
        "# model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim, device=device)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim, device=device)\n",
        "# model.embedding.weight.requires_grad = False\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=1.0e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
        "\n",
        "print (model)"
      ],
      "metadata": {
        "id": "DDsLV989s4av",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f8efae-aa1e-4d2a-f02e-aebf741c4c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq2seq(\n",
            "  (embedding): Embedding(29753, 50)\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(29753, 50)\n",
            "    (gru): GRU(50, 512, num_layers=2, dropout=0.5, bidirectional=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(29753, 50)\n",
            "    (gru): GRU(50, 512, num_layers=2, dropout=0.5)\n",
            "    (concat): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (out): Linear(in_features=512, out_features=29753, bias=True)\n",
            "    (attn): Attention()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, criterion, optimizer, clip=1.0):\n",
        "    # Put the model in training mode!\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)): # usando tqdm para mostrar el progreso\n",
        "\n",
        "        input_sequence = batch.input_sequence\n",
        "        output_sequence = batch.output_sequence\n",
        "        \n",
        "        target_tokens = output_sequence[0]\n",
        "        # target_tokens = torch.tensor(output_sequence[0], dtype=torch.long, device='cpu')\n",
        "        \n",
        "        \n",
        "        # zero out the gradient for the current batch\n",
        "        print (\"\\nAntes del error\")\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run the batch through our model\n",
        "        print (input_sequence)\n",
        "        print ('='*100)\n",
        "        print (output_sequence)\n",
        "        output = model(input_sequence, output_sequence)\n",
        "        print (\"\\nDespués del error\")\n",
        "        \n",
        "        # Throw it through our loss function\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        target_tokens = target_tokens[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(output, target_tokens)\n",
        "        \n",
        "        # Perform back-prop and calculate the gradient of our loss function\n",
        "        loss.backward()\n",
        "          \n",
        "        # Clip the gradient if necessary.          \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        print (f\"\\nBatch {idx+1} / {len(iterator)} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "XLtDydhktFnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "N_EPOCHS = 1\n",
        "CLIP = 50.0\n",
        "start = time.time()\n",
        "plot_train_loss = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
        "    avg_epochs =  epoch / N_EPOCHS\n",
        "    time_elapsed = timeSince(start, avg_epochs)\n",
        "    print(f'\\n{time_elapsed} ({epoch} {avg_epochs * 100}%) %{train_loss:.4f}\\n')\n",
        "    plot_train_loss.append(train_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "id": "ndYdIbH0tNeh",
        "outputId": "060af332-5a97-4535-8211-89a1afca1b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/848 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Antes del error\n",
            "(tensor([[    2,     2,     2,  ...,     2,     2,     2],\n",
            "        [    7,     7,    15,  ...,   112,    18,   189],\n",
            "        [  599,   252,   215,  ...,     4, 25186,    38],\n",
            "        ...,\n",
            "        [  638,    25,   180,  ...,   164,     0,   220],\n",
            "        [   21,     4,     4,  ...,     4,     8,     4],\n",
            "        [    3,     3,     3,  ...,     3,     3,     3]], device='cuda:0'), tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10], device='cuda:0'))\n",
            "====================================================================================================\n",
            "(tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
            "        [ 76,   7,  14,  ...,   6,  47,  77],\n",
            "        [  5, 121,  11,  ...,  20, 145,   8],\n",
            "        ...,\n",
            "        [  1,   1,   1,  ...,   1,   1,   1],\n",
            "        [  1,   1,   1,  ...,   1,   1,   1],\n",
            "        [  1,   1,   1,  ...,   1,   1,   1]], device='cuda:0'), tensor([ 34,  43,  11,   7,  31,  14,  19,  10,  11,  22,  15,  18,  20,  13,\n",
            "         10,   6,   6,  38,   8,   7,  12,  23,   6,  16,  23,   6,  10,   9,\n",
            "          4,  66,   6,  18,   4,   4,  16,  10,  34,  27,  18,  13,   8,  11,\n",
            "          4,   8,  10,  41,  10,  16,   8,   9,   5,   7,  10,  18,  45,  17,\n",
            "         24,   8,  15,   4,  12,  26,   5,  14,  16,   9,   7,  24,  96,  13,\n",
            "         10,  12,  93,  10,  17,   9,   5,   6,  14,   5,   8,   5,  33,   9,\n",
            "         15,   5,  14,   8,  44,   4,  38,  12,   8,  10,  12,  18,   7,  14,\n",
            "          9,   6,   4,  35,  16,  14,  13,   5,  24,  24,  10, 132,  10,   8,\n",
            "         28,  12,  10,   7,  14,  26,   8,  13,  18,   8,  21,   4,  30,  15,\n",
            "         19,  17], device='cuda:0'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/848 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-e6ddb3a6589e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mavg_epochs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mepoch\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeSince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-b0d885828be2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion, optimizer, clip)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDespués del error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-2973122c62e1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sequence, output_sequence, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#tensor to store decoder outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 7.39 GiB already allocated; 1.59 GiB free; 9.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# graficar las perdidas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print (f\"Epochs: {len(plot_train_loss)}\")\n",
        "\n",
        "# Graficar accuracy y loss\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "plt.plot(plot_train_loss, label='Training loss', marker='o', color='orange')\n",
        "plt.title('Losses', fontsize=15)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.savefig('/content/train_loss.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0bGNgZeqpTbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados"
      ],
      "metadata": {
        "id": "_8HZ1QmMtYGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def translate_sentence(model, sentence, nlp):\n",
        "    model.eval()\n",
        "    \n",
        "    tokenized = nlp(sentence) \n",
        "    \n",
        "    tokenized = ['<sos>'] + [t.lower_ for t in tokenized] + ['<eos>']\n",
        "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized] \n",
        "    \n",
        "    sentence_length = torch.LongTensor([len(numericalized)]).to(model.device) \n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device) \n",
        "    \n",
        "    translation_tensor_logits = model((tensor, sentence_length), None, 0) \n",
        "    \n",
        "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
        "    translation = [TEXT.vocab.itos[t] for t in translation_tensor]\n",
        " \n",
        "    # Start at the first index. We don't need to return the <sos> token...\n",
        "    translation = translation[1:]\n",
        "    return translation, translation_tensor_logits\n",
        "\n",
        "sentence = \"tell me a fun fact\"\n",
        "response, logits = translate_sentence(model, sentence, nlp)\n",
        "print(\" \".join(response))"
      ],
      "metadata": {
        "id": "NdT2TFf-tZvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}