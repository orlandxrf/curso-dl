{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orlandxrf/curso-dl/blob/main/notebooks/NeuronaArtificial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzcwP8baQvLJ"
      },
      "source": [
        "# Problema\n",
        "---\n",
        "Problema: El fin de semana vas a ver el final de tu serie favorita de `Netflix`, para ello tienes planeado comer `nachos` con mucho queso. Además tienes que `estudiar` para `aprobar` el examen parcial de Aprendizaje Profundo del día lunes. Lo que se desea es que la neurona nos indique el posible resultado (salida) de $Y$ dadas las entradas $X$.<br>\n",
        "<br>\n",
        "$X_{1} = \\{0, 1\\};$ $1$ si ves tu serie, $0$ si no la ves<br>\n",
        "$X_{2} = \\{0, 1\\};$ $1$ comes nachos, $0$ no hay nachos<br>\n",
        "$X_{3} = \\{0, 1\\};$ $1$ estudias para el examen, $0$ no estudias para el examen<br>\n",
        "<br>\n",
        "$Y_{1}= \\{0: reprobar, 1:aprobar\\}$<br>\n",
        "<br>\n",
        "Para calcular la salida, el umbral definido esta dado bajo las siguientes condiciones:<br>\n",
        "<br>\n",
        "$output = \\begin{cases} 0, & \\text{si } \\Sigma_{i}w_{i}X_{i} \\leq 5\\\\ 1, & \\text{si } \\Sigma_{i}w_{i}X_{i} > 5 \\end{cases}$<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7O5cdhVQvLO",
        "outputId": "dd26f099-5fc3-4461-eec1-27e40827e4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "X1        X2        X3        Y\n",
            "-----------------------------------\n",
            "0         0         0         0\n",
            "1         0         0         0\n",
            "0         1         0         0\n",
            "1         1         0         0\n",
            "0         0         1         1\n",
            "1         0         1         1\n",
            "0         1         1         1\n",
            "1         1         1         1\n",
            "------------------------------------------\n",
            "Netflix     nachos    estudiar    aprobar\n",
            "------------------------------------------\n",
            "NO          NO          NO          NO\n",
            "SI          NO          NO          NO\n",
            "NO          SI          NO          NO\n",
            "SI          SI          NO          NO\n",
            "NO          NO          SI          SI\n",
            "SI          NO          SI          SI\n",
            "NO          SI          SI          SI\n",
            "SI          SI          SI          SI\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def umbral(z):\n",
        "    if z > 4.0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "def neurona(x, w):\n",
        "    z = 0\n",
        "    for i in range(len(x)):\n",
        "        z += x[i] * w[i] # w1 * X1 + w2 * X2 + w3 * X3\n",
        "    a = umbral(z)\n",
        "    return a\n",
        "\n",
        "# definimos los valores de entrada\n",
        "X = [#  X1,  X2,  X3\n",
        "        [0, 0, 0],\n",
        "        [1, 0, 0],\n",
        "        [0, 1, 0],\n",
        "        [1, 1, 0],\n",
        "        [0, 0, 1],\n",
        "        [1, 0, 1],\n",
        "        [0, 1, 1],\n",
        "        [1, 1, 1],\n",
        "]\n",
        "\n",
        "# se establece la prioridad que le damos a cada entrada (pesos)\n",
        "w = [\n",
        "    3, # quiero ver el final de la serie de\n",
        "    1, # no importa si no hay nachos para comer\n",
        "    5, # el examen es mi prioridad\n",
        "]\n",
        "\n",
        "print ('-'*35)\n",
        "print (f\"{'X1':10}{'X2':10}{'X3':10}Y\")\n",
        "print ('-'*35)\n",
        "\n",
        "for i in range(len(X)):\n",
        "    Y = neurona(X[i], w)\n",
        "    print (f\"{X[i][0]:0}{X[i][1]:10}{X[i][2]:10}{int(Y):10}\")\n",
        "\n",
        "labels = {0:'NO', 1:'SI'}\n",
        "\n",
        "print ('-'*42)\n",
        "print (f\"{'Netflix':12}{'nachos':10}{'estudiar':12}aprobar\")\n",
        "print ('-'*42)\n",
        "\n",
        "for i in range(len(X)):\n",
        "    Y = neurona(X[i], w)\n",
        "    print (f\"{labels[X[i][0]]:12}{labels[X[i][1]]:12}{labels[X[i][2]]:12}{labels[int(Y)]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aSKnYBuQvLR"
      },
      "source": [
        "# Neurona Artificial\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7k2TF-zQvLR"
      },
      "source": [
        "La neurona artificial es un modelo simplificado de la neurona natural, la cual trata de imitar 3 aspectos principales:\n",
        "\n",
        "*   La fuerza sináptica que pondera los impulsos recibidos\n",
        "*   La acumulación de estos impulsos ponderados\n",
        "*   La activación de la neurona que produce un impulso de respuesta a su salid\n",
        "\n",
        "La primera neurona artificial fue la llamada Unidad de Umbral Lineal, propuesta en 1943 por Warren McCulloch y Walter Pitts, la cual presupone que tanto los valores de los atributos de entrada como los valores de salida son binarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKXkxuAsQvLS"
      },
      "source": [
        "## Unidad de Umbral Lineal\n",
        "La operación que lleva a cabo una neurona artificial está dada por la suma pesada evaluada en una función de activación $\\phi$. Una de las primeras funciones de activación utilizadas fue la **escalón unitario**, definida como:<br>\n",
        "<br>\n",
        "$\\phi(x) = \\begin{cases} 1, & \\text{si } x \\geq 0\\\\0, & \\text{en caso contrario}\\end{cases}$<br>\n",
        "<br>\n",
        "Esta se puede llevar a cabo con la siguiente función de Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_gwu__tHQvLS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def funcion_escalon(z):\n",
        "  if z >= 0.0:\n",
        "    return 1.0\n",
        "  else:\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OH4hUlSQvLT"
      },
      "source": [
        "La suma pesada simplemente consiste en multiplicar cada entrada por su correspondiente peso y sumarle el sesgo. Esto lo podemos expresar como:<br>\n",
        "<br>\n",
        "$z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots + w_d \\cdot x_d + b$<br>\n",
        "<br>\n",
        "En su forma vectorial:<br>\n",
        "<br>\n",
        "$z = \\mathbf{w}^T \\mathbf{x} + b$<br>\n",
        "<br>\n",
        "Para realizar esto en Python, podemos usar la función `dot` de `numpy` de la siguiente manera `z = np.dot(w.T, x) + b`. Así, la operación de la neurona completa sería:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2xAN-0NoQvLT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def neurona(x, w, b):\n",
        "  z = np.dot(w.T, x) + b\n",
        "  a = funcion_escalon(z)\n",
        "\n",
        "  return a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neurona Artificial para calcular la operación lógica OR ($\\lor$)\n",
        "La tabla de verdad de la operación lógica OR es la siguiente:<br>\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ----- | :----:| :--:|\n",
        "|0 |0 | 0 |\n",
        "|0 |1 | 1 |\n",
        "|1 |0 | 1 |\n",
        "|1 |1 | 1 |\n",
        "\n",
        "Donde $y=x_1 \\lor x_2$. La neurona recibe 2 valores binarios como entrada y produce un valor binario como salida. Lo que se desea calcular es:<br>\n",
        "<br>\n",
        "$\\hat{y} = \\phi(w_1 \\cdot x_1 + w_2 \\cdot x_2 + b)$<br>\n",
        "<br>\n",
        "Los posibles valores para aproximar la operación OR son: $w_{1}=3$, $w_{2}=4$ y $b=-2$."
      ],
      "metadata": {
        "id": "aiVS5b-rR68V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# valores de X1 y X2\n",
        "X = np.array([\n",
        "  [0., 0.],\n",
        "  [0., 1.],\n",
        "  [1., 0.],\n",
        "  [1., 1.]\n",
        "])\n",
        "\n",
        "# establecer los pesos de w1 y w2\n",
        "w = np.array([3, 4]).T # la matriz transpuesta de los pesos\n",
        "\n",
        "# definir el bias\n",
        "b = -2\n",
        "\n",
        "print ('-'*21)\n",
        "print (f\"{'X1':8}{'X2':8}{'y_hat':8}\")\n",
        "print ('-'*21)\n",
        "\n",
        "for i in range(X.shape[0]): # X.shape = (4, 2)\n",
        "    y_hat = neurona(X[i, :], w, b)\n",
        "    # print ('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))\n",
        "    print (f\"{X[i,0]:0}{X[i,1]:8}{y_hat:8}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7Cwr0QaWh3k",
        "outputId": "a57013a9-c60d-4389-eecd-b71207b34ef0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "X1      X2      y_hat   \n",
            "---------------------\n",
            "0.0     0.0     0.0\n",
            "0.0     1.0     1.0\n",
            "1.0     0.0     1.0\n",
            "1.0     1.0     1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio**: Función lógica AND y NAND\n",
        "Tabla de verdad de la función lógica AND ($\\land$):<br>\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ----- | :----:| :--:|\n",
        "|0 |0 | 0 |\n",
        "|0 |1 | 0 |\n",
        "|1 |0 | 0 |\n",
        "|1 |1 | 1 |\n",
        "\n",
        "Tabla de verdad de la función lógica NAND:<br>\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ----- | :----:| :--:|\n",
        "|0 |0 | 1 |\n",
        "|0 |1 | 1 |\n",
        "|1 |0 | 1 |\n",
        "|1 |1 | 0 |\n",
        "\n",
        "Encuentre los valores para $w_1$, $w_2$ y $b$ para cada una de las funciones lógicas."
      ],
      "metadata": {
        "id": "2SiTnPwoaHm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# escriba aquí sus soluciones para las funciones AND y NAND"
      ],
      "metadata": {
        "id": "y5UEhNxjdbe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron"
      ],
      "metadata": {
        "id": "5ooc_FYBe-as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(X, y, n_epochs = 10):\n",
        "  w = np.zeros(X.shape[1]) # pesos\n",
        "  b = 0 # bias\n",
        "  for i in range(n_epochs):\n",
        "    suma_error = 0.0\n",
        "    for j in range(X.shape[0]):\n",
        "      y_hat = neurona(X[j], w, b)\n",
        "      \n",
        "      # se calcula el \"error\" entre la salida real actual \"y[j]\" y la salida objetivo \"y_hat\".\n",
        "      error = y[j] - y_hat\n",
        "\n",
        "      # sumar el error al peso actual\n",
        "      w += error * X[j]\n",
        "      # sumar el error al bias actual\n",
        "      b += error\n",
        "      \n",
        "      # sumatoria de los errores\n",
        "      suma_error += np.abs(error)\n",
        "\n",
        "    print(f\"Epoch {i}/{n_epochs} error: {suma_error / float(X.shape[0])}\")\n",
        "\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "Ie61fT_NfLal"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aproximando la función lógica OR ($\\lor$)\n"
      ],
      "metadata": {
        "id": "aTvfS4GuhOJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# valores de X1 y X2\n",
        "X = np.array([\n",
        "  [0., 0.],\n",
        "  [0., 1.],\n",
        "  [1., 0.],\n",
        "  [1., 1.],\n",
        "])\n",
        "\n",
        "# valor esperado \n",
        "y_or = np.array([\n",
        "  0.,\n",
        "  1.,\n",
        "  1.,\n",
        "  1.,\n",
        "]) \n",
        "\n",
        "# llamar función perceptrón para aproximar los valores\n",
        "w, b = perceptron(X, y_or, n_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrPoxuv9hlkg",
        "outputId": "d9c64887-53e0-44ab-fc4d-cbce6a540131"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10 error: 0.5\n",
            "Epoch 1/10 error: 0.5\n",
            "Epoch 2/10 error: 0.25\n",
            "Epoch 3/10 error: 0.0\n",
            "Epoch 4/10 error: 0.0\n",
            "Epoch 5/10 error: 0.0\n",
            "Epoch 6/10 error: 0.0\n",
            "Epoch 7/10 error: 0.0\n",
            "Epoch 8/10 error: 0.0\n",
            "Epoch 9/10 error: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mostrar los pesos identificados\n",
        "print (f\"w1:\\t{w[0]}\\nw1:\\t{w[1]}\\nb:\\t{b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsnNOOTEiecM",
        "outputId": "17e47e62-52ae-4a08-8d10-e83a6bbfbb3b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1:\t1.0\n",
            "w1:\t1.0\n",
            "b:\t-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verificar valores\n",
        "# utilizar los valores aprendidos con la neurona artificial\n",
        "print ('-'*21)\n",
        "print (f\"{'X1':8}{'X2':8}{'y_hat':8}\")\n",
        "print ('-'*21)\n",
        "\n",
        "for i in range(X.shape[0]): # X.shape = (4, 2)\n",
        "    y_hat = neurona(X[i, :], w, b)\n",
        "    # print ('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))\n",
        "    print (f\"{X[i,0]:0}{X[i,1]:8}{y_hat:8}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IxEaxi0jU3Q",
        "outputId": "aae2ebec-5b8b-49e6-ac95-a1a03c2991d6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "X1      X2      y_hat   \n",
            "---------------------\n",
            "0.0     0.0     0.0\n",
            "0.0     1.0     1.0\n",
            "1.0     0.0     1.0\n",
            "1.0     1.0     1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aproximando la función lógica AND"
      ],
      "metadata": {
        "id": "UgzoW5r8jzU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# los valores de X1 y X2 no cambian\n",
        "print (X)\n",
        "\n",
        "# definir valores esperados para AND\n",
        "y_and = np.array([\n",
        "  0.,\n",
        "  0.,\n",
        "  0.,\n",
        "  1.,\n",
        "])\n",
        "\n",
        "# llamar perceptron para aproximar los valores\n",
        "w, b = perceptron(X, y_and, n_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K11DcXMKj84i",
        "outputId": "eebe1abf-207a-477c-f50f-ad293bf1a501"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 1.]]\n",
            "Epoch 0/10 error: 0.5\n",
            "Epoch 1/10 error: 0.75\n",
            "Epoch 2/10 error: 0.75\n",
            "Epoch 3/10 error: 0.5\n",
            "Epoch 4/10 error: 0.25\n",
            "Epoch 5/10 error: 0.0\n",
            "Epoch 6/10 error: 0.0\n",
            "Epoch 7/10 error: 0.0\n",
            "Epoch 8/10 error: 0.0\n",
            "Epoch 9/10 error: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mostrar los pesos identificados para aproximar los valores de AND\n",
        "print (f\"w1:\\t{w[0]}\\nw1:\\t{w[1]}\\nb:\\t{b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-XXmscpk9lr",
        "outputId": "06728646-2e49-4729-d672-4494c56b36a3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1:\t2.0\n",
            "w1:\t1.0\n",
            "b:\t-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verificar valores\n",
        "# utilizar los valores aprendidos con la neurona artificial\n",
        "print ('-'*21)\n",
        "print (f\"{'X1':8}{'X2':8}{'y_hat':8}\")\n",
        "print ('-'*21)\n",
        "\n",
        "for i in range(X.shape[0]): # X.shape = (4, 2)\n",
        "    y_hat = neurona(X[i, :], w, b)\n",
        "    # print ('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))\n",
        "    print (f\"{X[i,0]:0}{X[i,1]:8}{y_hat:8}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6b40fd-d721-4388-c167-df761daaf217",
        "id": "iyc9ybGjlka8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "X1      X2      y_hat   \n",
            "---------------------\n",
            "0.0     0.0     0.0\n",
            "0.0     1.0     0.0\n",
            "1.0     0.0     0.0\n",
            "1.0     1.0     1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aproximando la función lógica NAND"
      ],
      "metadata": {
        "id": "7R742OZhlMNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# los valores de X1 y X2 no cambian\n",
        "print (X)\n",
        "\n",
        "# definir valores esperados para NAND\n",
        "y_and = np.array([\n",
        "  1.,\n",
        "  1.,\n",
        "  1.,\n",
        "  0.,\n",
        "])\n",
        "\n",
        "# llamar perceptron para aproximar los valores\n",
        "w, b = perceptron(X, y_and, n_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35817875-8331-418d-bb32-221fd895bfd6",
        "id": "SPXCDVEYlMNz"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 1.]]\n",
            "Epoch 0/10 error: 0.25\n",
            "Epoch 1/10 error: 0.75\n",
            "Epoch 2/10 error: 0.75\n",
            "Epoch 3/10 error: 0.5\n",
            "Epoch 4/10 error: 0.25\n",
            "Epoch 5/10 error: 0.0\n",
            "Epoch 6/10 error: 0.0\n",
            "Epoch 7/10 error: 0.0\n",
            "Epoch 8/10 error: 0.0\n",
            "Epoch 9/10 error: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mostrar los pesos identificados para aproximar los valores de NAND\n",
        "print (f\"w1:\\t{w[0]}\\nw1:\\t{w[1]}\\nb:\\t{b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbc72ad-805e-4868-a561-1a3dae41c5f1",
        "id": "xOp8jQOvlMN1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1:\t-2.0\n",
            "w1:\t-1.0\n",
            "b:\t2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verificar valores\n",
        "# utilizar los valores aprendidos con la neurona artificial\n",
        "print ('-'*21)\n",
        "print (f\"{'X1':8}{'X2':8}{'y_hat':8}\")\n",
        "print ('-'*21)\n",
        "\n",
        "for i in range(X.shape[0]): # X.shape = (4, 2)\n",
        "    y_hat = neurona(X[i, :], w, b)\n",
        "    # print ('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))\n",
        "    print (f\"{X[i,0]:0}{X[i,1]:8}{y_hat:8}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cc5ceb-c8bd-4a07-8aff-c075d005d0f2",
        "id": "qTpWL_tllnZ6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "X1      X2      y_hat   \n",
            "---------------------\n",
            "0.0     0.0     1.0\n",
            "0.0     1.0     1.0\n",
            "1.0     0.0     1.0\n",
            "1.0     1.0     0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Tarea 1\n",
        "\n",
        "* Del problema descrito al inicio del notebook, aproximar sus valores con lo aprendido hasta ahora.\n",
        "* Escribir un documento con su propuesta con resultados (pueden utilizar capturas de pantalla)  con su nombre y número de cuenta\n",
        "* Enviar de forma electrónica al correo de contacto\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1D-atE5L6jt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aproximación de funciones no lineales: XOR($\\oplus$)\n",
        "\n",
        "Minsky y Papert mostraron que una neurona LTU (Threshold Logic Unit) o LTU (Linear Threshold Unit) no puede aproximar de forma precisa una función no lineal como la compuerta XOR ():\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ----- | :----:| :--:|\n",
        "|0 |0 | 0 |\n",
        "|0 |1 | 1 |\n",
        "|1 |0 | 1 |\n",
        "|1 |1 | 0 |\n",
        "\n",
        "Sin embargo, es posible aproximar este tipo combinando múltiples LTU conectadas en red. Por ejemplo, es posible llevar a cabo la operación XOR con operaciones OR, AND y NAND:<br>\n",
        "<br>\n",
        "$x_1 \\mathbin{\\oplus} x_2 = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2)$<br>\n",
        "<br>\n",
        "Se define en el siguiente método:"
      ],
      "metadata": {
        "id": "eZjlGgHcda3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multicapa(x, W1, b1, W2, b2):\n",
        "  # np.vectorize = define una función vectorizada que toma una secuencia anidada de objetos o matrices numpy como entradas\n",
        "  # y devuelva una sola matriz numpy o una tupla de matrices numpy\n",
        "  escalon_vectorial = np.vectorize(funcion_escalon)\n",
        "  \n",
        "  # realiza la multiplicación de las entradas de X1 y X2 con los pesos definidos en W1 y suma el bias\n",
        "  a = escalon_vectorial(np.dot(W1.T, x) + b1)\n",
        "\n",
        "  #finalmente regresa la multiplicación de los pesos definidos en W2 con el resultado de la multiplicación anterior y suma el bias\n",
        "  return escalon_vectorial(np.dot(W2.T, a) + b2)"
      ],
      "metadata": {
        "id": "XhZMhh0LvmU7"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encontrando los valores de pesos y sesgos adecuados, podemos usar esta función para aproximar la operación XOR. Previamente, ya se han encontrado los pesos y sesgos para las operaciones OR, AND y NAND, por lo que podemos usar estas neuronas con sus correspondientes pesos y sesgos.<br>\n",
        "<br>\n",
        "La red tendría 2 neuronas conectadas a las entradas que realizan las operaciones OR ($w_{11}^{\\{1\\}} = 10$, $w_{12}^{\\{1\\}} = 10$ y $b_1^{\\{1\\}} = -5$) y NAND ($w_{21}^{\\{1\\}} = -10$, $w_{22}^{\\{1\\}} = -10$ y $b_2^{\\{1\\}} = 15$) respectivamente.<br>\n",
        "<br>\n",
        "La salida de estas 2 neuronas estarían conectadas a una tercera neurona que realiza la operacioón AND ($w_{11}^{\\{2\\}} = 10$, $w_{12}^{\\{2\\}} = 10$ y $b_1^{\\{2\\}} = -15$).<br>\n",
        "<br>\n",
        "En su forma matricial:<br>\n",
        "\n",
        "$\\mathbf{W}^{\\{1\\}} = \\begin{bmatrix} 10 & -10\\\\, 10 & -10, \\\\ \\end{bmatrix}$\n",
        "\n",
        "$\\mathbf{b}^{\\{1\\}} = \\begin{bmatrix} -5 & 15 \\end{bmatrix}$\n",
        "\n",
        "$\\mathbf{W}^{\\{2\\}} = \\begin{bmatrix} 10\\\\ 10 \\end{bmatrix}$\n",
        "\n",
        "$\\mathbf{b}^{\\{2\\}} = \\begin{bmatrix} -15 \\end{bmatrix}$\n"
      ],
      "metadata": {
        "id": "PnP0RjKkwIld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# los valores de X1 y X2 siguen siendo los mismos\n",
        "print (f\"{X}\\n\")\n",
        "\n",
        "# establecer los valores esperados\n",
        "y_xor = np.array([0., 1., 1., 0.])\n",
        "\n",
        "# establecer los pesos de OR y NAND, así como su bias respectivamente\n",
        "W1 = np.array([[10, -10], [10, -10]])\n",
        "b1 = np.array([-5, 15])\n",
        "\n",
        "# establecer los pesos de AND y su bias\n",
        "W2 = np.array([[10], [10]])\n",
        "b2 = np.array([-15])\n",
        "\n",
        "print(f'W1:\\t[{W1[0,:]}{W1[1,:]}]\\nb1:\\t{b1}\\n')\n",
        "print(f'W2:\\t[{W2[0]}{W2[1]}]\\nb_2:\\t{b2}\\n')\n",
        "\n",
        "\n",
        "print('-'*29)\n",
        "print(f\"{'x1':8}{'x2':8}{'y':8}{'y_hat':8}\")\n",
        "print('-'*29)\n",
        "\n",
        "for i in range(X.shape[0]): # X.shape = (4, 2)\n",
        "  y_hat = multicapa(X[i], W1, b1, W2, b2)\n",
        "  print(f\"{X[i, 0]}{X[i, 1]:8}{y_xor[i]:8}{y_hat[0]:8}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXk5Y_jpzSfS",
        "outputId": "872ff704-6227-40bd-bd1b-9250d2757c6b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 1.]]\n",
            "\n",
            "W1:\t[[ 10 -10][ 10 -10]]\n",
            "b1:\t[-5 15]\n",
            "\n",
            "W2:\t[[10][10]]\n",
            "b_2:\t[-15]\n",
            "\n",
            "-----------------------------\n",
            "x1      x2      y       y_hat   \n",
            "-----------------------------\n",
            "0.0     0.0     0.0     0.0\n",
            "0.0     1.0     1.0     1.0\n",
            "1.0     0.0     1.0     1.0\n",
            "1.0     1.0     0.0     0.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "4837d292e4b9126da116e06af1f811e056f226b336016b6f4bf6f734c80207f6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "NeuronaArtificial.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}