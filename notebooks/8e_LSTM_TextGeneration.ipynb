{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8e_LSTM_TextGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMO39iP4cZ3gmAVZZyZ6PDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orlandxrf/curso-dl/blob/main/notebooks/8e_LSTM_TextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genereación de Texto usando una LSTM\n",
        "Para esta tarea se utilizó el libro [Memorias de un vigilante by José S. Alvarez](https://www.gutenberg.org/ebooks/19543).\n",
        "\n",
        "[Libros en idioma español del proyecto Gutenberg](https://www.gutenberg.org/browse/languages/es)"
      ],
      "metadata": {
        "id": "PLJbw0t4f59M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02MUSfCPfy_v"
      },
      "outputs": [],
      "source": [
        "# Definir las bibliotecas a usar\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar el libro\n",
        "import os\n",
        "\n",
        "URL = 'https://raw.githubusercontent.com/orlandxrf/curso-dl/main/data/memoriasDeUnVigilante.txt'\n",
        "data_folder = 'data'\n",
        "filepath = os.path.join(data_folder, 'memoriasDeUnVigilante.txt')\n",
        "print (f\"filepath:\\t{filepath}\")\n",
        "\n",
        "# crear carpeta para almacenar el conjunto de datos\n",
        "! mkdir {data_folder}\n",
        "\n",
        "# descargar conjunto de datos y alamcenar\n",
        "! wget -nc {URL} -O {filepath}\n",
        "\n",
        "# comprobar descarga\n",
        "! ls -lh data/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7UHM07hbul",
        "outputId": "55e4265b-6e8f-4a9a-9781-5203316130af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepath:\tdata/memoriasDeUnVigilante.txt\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "File ‘data/memoriasDeUnVigilante.txt’ already there; not retrieving.\n",
            "-rw-r--r-- 1 root root 128K Mar 15 01:50 data/memoriasDeUnVigilante.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# leer el libro\n",
        "text = \"\"\n",
        "with open('data/memoriasDeUnVigilante.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        if len(line.strip()) > 0: # evitar leer líneas en blanco\n",
        "            text += line\n",
        "f.close()"
      ],
      "metadata": {
        "id": "RueAm6UOjgxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (text[0:1000])\n",
        "print (f\"\\nLongitud del texto:\\t{len(text):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0pjq1GwkAzN",
        "outputId": "11dc1cfe-46fb-4ab5-ab41-5a46f8e231f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "DOS PALABRAS\n",
            "No abrigo la esperanza de que mis recuerdos lleguen a constituir un\n",
            "libro interesante; los he escrito en mis ratos de ocio y no tengo\n",
            "pretensiones de filósofo, ni de literato.\n",
            "No obstante, creo que nadie que me lea perderá su tiempo, pues, por lo\n",
            "menos, se distraerá con casos y cosas que quizás habrá mirado sin ver y\n",
            "que yo en el curso de mi vida me vi obligado a observar en razón de mi\n",
            "temperamento o de mis necesidades.\n",
            "II\n",
            "EN LOS UMBRALES DE LA VIDA\n",
            "Mi nacimiento fue como el de tantos, un acontecimiento natural, de esos\n",
            "que con abrumadora monotonía y constante regularidad se producen\n",
            "diariamente en los ranchos de nuestras campañas desiertas.\n",
            "Para mi padre, fui seguramente una boca más que alimentar, para mi\n",
            "madre, una preocupación que se sumaba a las ocho iguales que ya tenía, y\n",
            "para los perros de la casa y para los pajaritos del monte que nos\n",
            "rodeaba, una promesa segura de cascotazos y mortificaciones que\n",
            "comenzaría a cumplirse dentro de los tres años de la fecha y dur\n",
            "\n",
            "Longitud del texto:\t126,820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crear vocabularios\n",
        "chars = tuple(set(text))\n",
        "\n",
        "print (f\"Longitud del vocabulario:\\t{len(chars)}\")\n",
        "print (f\"Vocabulario:\\t{chars}\")\n",
        "\n",
        "itos = dict(enumerate(chars))\n",
        "stoi = {ch: ii for ii, ch in itos.items()}\n",
        "\n",
        "print (f\"\\nLongitud stoi:\\t{len(stoi)}\")\n",
        "print (f\"Longitud itos:\\t{len(itos)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be9OR1jTkMrB",
        "outputId": "cee495fd-22de-4f25-cd0e-b7505141c410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del vocabulario:\t95\n",
            "Vocabulario:\t('o', '0', 'Y', ',', '«', 'k', '4', 'x', 'á', 'R', '&', 'f', '_', '*', '\"', 'M', 'ú', 's', 'T', 'v', 'Á', '?', 'L', 'B', 'Ñ', 'c', 'P', 'a', 'W', 'N', '[', ' ', '\\n', 'É', '¡', '6', ')', 'h', 'Q', 'Ó', 'J', 'ñ', 'U', 'º', 'S', 'l', '3', 't', '¿', 'ü', '(', 'D', ';', 'I', 'Z', 'm', 'V', '5', 'F', 'd', 'O', 'ó', 'w', '9', '.', 'z', '2', 'p', 'X', 'r', 'n', 'H', 'i', 'E', 'g', 'é', ':', 'ª', 'C', '1', 'u', '!', '7', 'q', 'A', 'b', '»', 'y', 'G', 'e', '8', 'í', 'j', ']', '-')\n",
            "\n",
            "Longitud stoi:\t95\n",
            "Longitud itos:\t95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificar el texto, transformar los caracteres a sus índices\n",
        "encoded_text = np.array([stoi[ch] for ch in text])\n",
        "\n",
        "print (f\"Longitud del texto codificado: {len(encoded_text):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlc3qg1Zk5y_",
        "outputId": "12635794-3c46-425a-d167-ccafc586aee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto codificado: 126,820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# función para hacer el one-hot-encode\n",
        "def oneHotEncode(arr, n_labels):\n",
        "    \n",
        "    # inicializar la matriz en ceros\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # rellenar los elementos apropiados con unos\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # finalmente hacer reshaping para volver a la matriz original\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "-O4Wfzzbld45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verificar la función\n",
        "sequence = encoded_text[15:101] \n",
        "print (f\"secuencia codificada:\\t{sequence}\\n\")\n",
        "\n",
        "words = ''.join([itos[item] for item in sequence])\n",
        "print (f\"secuencia decodificada:\\t{words}\")\n",
        "\n",
        "one_hot = oneHotEncode(sequence, len(chars))\n",
        "\n",
        "tmp_chars = tuple(set(words))\n",
        "print (f\"\\ntmp_chars:\\t{tmp_chars}\\tLongitud:\\t{len(tmp_chars)}\")\n",
        "print (f\"\\nLongitud de caracteres en la secuencia:\\t{len(words)}\")\n",
        "\n",
        "tmp = one_hot.tolist()\n",
        "\n",
        "print (f\"\\none_hot shape:\\t{one_hot.shape}\\n\")\n",
        "\n",
        "for i, item in enumerate(one_hot.tolist()):\n",
        "  if i==4: break\n",
        "  print (f\"{i}\\t{item}\")\n",
        "\n",
        "print (f\"\\n' ':{stoi[' ']}\\t'a':{stoi['a']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzhvig_tlu2w",
        "outputId": "0caf196a-035f-49b5-c5a4-23198cf49dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "secuencia codificada:\t[29  0 31 27 85 69 72 74  0 31 45 27 31 89 17 67 89 69 27 70 65 27 31 59\n",
            " 89 31 83 80 89 31 55 72 17 31 69 89 25 80 89 69 59  0 17 31 45 45 89 74\n",
            " 80 89 70 31 27 31 25  0 70 17 47 72 47 80 72 69 31 80 70 32 45 72 85 69\n",
            "  0 31 72 70 47 89 69 89 17 27 70 47 89 52]\n",
            "\n",
            "secuencia decodificada:\tNo abrigo la esperanza de que mis recuerdos lleguen a constituir un\n",
            "libro interesante;\n",
            "\n",
            "tmp_chars:\t('o', 'r', 't', 'n', 'i', 'c', 'g', 'a', 'N', 'u', ' ', '\\n', 'q', 'b', ';', 'm', 'd', 'e', 's', 'z', 'p', 'l')\tLongitud:\t22\n",
            "\n",
            "Longitud de caracteres en la secuencia:\t86\n",
            "\n",
            "one_hot shape:\t(86, 95)\n",
            "\n",
            "0\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "1\t[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "2\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "3\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "\n",
            "' ':31\t'a':27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "En la siguiente Figura se ilustra el objetivo al que se quiere llegar en este notebook. \n",
        "\n",
        "![Forward](https://drive.google.com/uc?id=1NIo-6yufX5WLHziaAKmyunl0SXpvgdci)\n",
        "\n",
        "Se desea predecir el siguiente carácter dado que se tiene el anterior."
      ],
      "metadata": {
        "id": "4Ogv7bW-oNaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir los mini-batches\n",
        "\n",
        "La división del conjunto de entrenamiento y validación se realiza antes de obtener los mini batches. Una vez hecho esto.\n",
        "\n",
        "Se calcula cúal será el tamaño total del mini batch (batch size * sequence length)\n",
        "\n",
        "Después se puede conocer cuantos mini batches se pueden obtener dada la longitud de la matriz de entrada entre el tamaño total del mini batch (matrix input length // batch size total)\n",
        "\n",
        "Una vez que se definió el tamaño del conjunto de entrenamiento, se obtienen los valores para las \"etiquetas\" y:\n",
        "\n",
        "Dado un mini batch de 10 (batches) x 10 (longitud de la secuencia)\n",
        "\n",
        "<pre>\n",
        "x:[\n",
        " [64 57 61 16 54 71 94 15 26 15]\n",
        " [28 73 28 71 43 17 14 73 28 73]\n",
        " [19 43 17 14  0 71 28  4 71 19]\n",
        " [28 19 14 28 71 28 37 73 43 71]\n",
        " [44 37 43 29 57  6 17 28 71 14]\n",
        " [88 28 42 71 19 43 71 17 38 19]\n",
        " [ 4 71 20 37 17 71 75  6 43 71]\n",
        " [71 44 43 71 28  4 78  6 37 43]\n",
        " [18 28 71  6 17 28 71 30 28 27]\n",
        " [38 44 28 71 73 43 28 14 28 44]\n",
        "]\n",
        "\n",
        "x.shape: (10, 10\n",
        "</pre>\n",
        "\n",
        "Obtener los valores para y. Como se pretende predecir los valores (caracteres) siguientes y será formado de la siguiente manera:\n",
        "\n",
        "<pre>\n",
        "y:[\n",
        " [57 61 16 54 71 94 15 26 15 32]\n",
        " [73 28 71 43 17 14 73 28 73 71]\n",
        " [43 17 14  0 71 28  4 71 19 43]\n",
        " [19 14 28 71 28 37 73 43 71 44]\n",
        " [37 43 29 57  6 17 28 71 14 28]\n",
        " [28 42 71 19 43 71 17 38 19 71]\n",
        " [71 20 37 17 71 75  6 43 71  4]\n",
        " [44 43 71 28  4 78  6 37 43 17]\n",
        " [28 71  6 17 28 71 30 28 27 28]\n",
        " [44 28 71 73 43 28 14 28 44 28]\n",
        "]\n",
        "\n",
        "y.shape: (10, 10)\n",
        "</pre>\n"
      ],
      "metadata": {
        "id": "ShrPLmED0JBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definir una función para obtener las secuencias\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    \"\"\"crea un generador que devuelve lotes de tamaño batch_size x seq_length de una matriz dada\n",
        "    arr: matriz de la que desea hacer lotes\n",
        "    batch_size: tamaño del lote, el número de secuencias por lote\n",
        "    seq_length: número de caracteres codificados en una secuencia\n",
        "    \"\"\"\n",
        "    \n",
        "    # obtener el tamaño total del batch\n",
        "    batch_size_total = batch_size * seq_length\n",
        "\n",
        "    # número total de lotes que podemos hacer\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # solo mantener los caracteres suficientes para hacer lotes completos\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    \n",
        "    # reshaping en filas del tamaño de lote\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterar a través de la matriz, una secuencia a la vez\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # las características (entrada)\n",
        "        x = arr[:, n:n+seq_length]\n",
        "\n",
        "        # los objetivos, DESPLAZADOS POR UNO\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "OSy8lqRmsHsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # crear el conjunto de entrenamiento y validación\n",
        "\n",
        "# val_frac = 0.1\n",
        "# val_idx = int( len(encoded_text) * (1-val_frac) )\n",
        "# print (f\"encoded_text:\\t{len(encoded_text):,}\") # 100%\n",
        "# print (f\"val_idx:\\t{val_idx:,}\") # 10 %\n",
        "\n",
        "# train_data, val_data = encoded_text[:val_idx], encoded_text[val_idx:]\n",
        "\n",
        "# for x, y in get_batches(val_data, batch_size=10, seq_length=10):\n",
        "#   print (x.shape, y.shape)\n",
        "#   print (x.tolist())\n",
        "#   print (y.tolist())\n",
        "#   break\n",
        "\n",
        "# res = get_batches(train_data, batch_size=10, seq_length=10)\n",
        "# print (f\"\\n{res}\")"
      ],
      "metadata": {
        "id": "K_NzhDFcsPI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verificar si tenemos cudas disponibles\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Se puede entrenar con GPU!')\n",
        "else: \n",
        "    print('No hay GPU disponible, entrenar en CPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqlaGS5j7ogc",
        "outputId": "ee0e220d-ca22-45af-e438-9490f0a9d6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No hay GPU disponible, entrenar en CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir el modelo de la LSTM"
      ],
      "metadata": {
        "id": "9fgspa_Z8Ur1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMtextGeneration(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # capa lstm\n",
        "        self.lstm=nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # capa dropout\n",
        "        self.dropout=nn.Dropout(drop_prob)\n",
        "        \n",
        "        # capa totalmente conectada FC\n",
        "        self.fc=nn.Linear(n_hidden,len(self.chars))\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        # obtenger la salida y el nuevo estado oculto del lstm\n",
        "        # x --> [10, 10, 95]\n",
        "        r_output, hidden = self.lstm(x, hidden) # r_output --> [10, 10, 512]\n",
        "        \n",
        "        # pasar la salida por la capa Dropout\n",
        "        out = self.dropout(r_output) # out --> [10, 10, 512]\n",
        "      \n",
        "\n",
        "        # hacer reshaping\n",
        "        out = out.contiguous().view(-1, self.n_hidden) # out --> [100, 512]\n",
        "        \n",
        "        # pasar la salida a la capa FC\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\" Inicializar el estado oculto \"\"\"\n",
        "        \n",
        "        # crear dos nuevos tensores con tamaños n_layers x batch_size x n_hidden\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "metadata": {
        "id": "3UQbiUgq8XPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion para el entrenamiento\n"
      ],
      "metadata": {
        "id": "bk3yf0Vm81zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    \"\"\" \n",
        "    model: instancia de la red LSTMtextGeneration\n",
        "    data: datos de texto para entrenar la red\n",
        "    epochs: número de épocas a entrenar\n",
        "    batch_size: número de minisecuencias por minilote, también conocido como tamaño del lote\n",
        "    seq_length: número de secuencias de caracteres por mini-lote\n",
        "    lr: taza de aprendizaje\n",
        "    clip: recorte de degradado\n",
        "    val_frac: fracción de datos que se espera para la validación\n",
        "    print_every: número de pasos para la pérdida de entrenamiento y validación de impresión\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # crear datos de entrenamiento y validación\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    # print (f\"conjunto de entrenamiento: {len(data):,}\\t{data.shape}\")\n",
        "    # print (f\"conjunto de validación: {len(val_data):,}\\t{val_data.shape}\")\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        model.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(model.chars)\n",
        "    loss_epoch_train, loss_epoch_val = [], []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        # inicializar estado oculto\n",
        "        h = model.init_hidden(batch_size)\n",
        "        \n",
        "        # obtener los mini batches del conjunto de entrenamiento y validación\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # pasar vectores a one-hot encode\n",
        "            x = oneHotEncode(x, n_chars)\n",
        "\n",
        "            # convertir los datos a tensores\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            # si hay cuda disponible definir los datos así\n",
        "            if (train_on_gpu): inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            \n",
        "            # crear nuevas variables para el estado oculto, de lo contrario, se retrodecería a través de todo el historial de entrenamiento\n",
        "            h = tuple([each.data for each in h]) # n_layers --> 2, [2, 10, 512]\n",
        "\n",
        "            # resetear gradientes acumulados cero\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # obtener la salida del modelo\n",
        "            output, h = model(inputs, h)\n",
        "            \n",
        "            # calcular la pérdida\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "\n",
        "            # retropropagación \n",
        "            loss.backward()\n",
        "            \n",
        "            # `clip_grad_norm` ayuda a prevenir el problema del gradiente explosivo en RNN / LSTM\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            \n",
        "            # actualizar los pesos\n",
        "            optimizer.step()\n",
        "            \n",
        "            # estadísticas de las perdidas\n",
        "            if counter % print_every == 0: # verificar cada cuanto se imprimie la información\n",
        "                \n",
        "                # obtener la pérdida del conjunto de validación\n",
        "                val_h = model.init_hidden(batch_size) # obtener estados ocultos\n",
        "\n",
        "                val_losses = [] # almacenar perdidas\n",
        "                \n",
        "                model.eval()\n",
        "\n",
        "                # no utilizar cálculos del gradiente\n",
        "                with torch.no_grad():\n",
        "                \n",
        "                    # iterar con el conjunto de validación\n",
        "                    for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "\n",
        "                        # obtener datos con one-hot encode\n",
        "                        x = oneHotEncode(x, n_chars)\n",
        "\n",
        "                        # transformar datos a tensores\n",
        "                        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                        \n",
        "                        # crear nuevas variables para el estado oculto, de lo contrario, se retrodecería a través de todo el historial de entrenamiento\n",
        "                        val_h = tuple([each.data for each in val_h])\n",
        "                        \n",
        "                        inputs, targets = x, y\n",
        "\n",
        "                        # si hay cuda disponible definir los datos así\n",
        "                        if(train_on_gpu): inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                        # obtener la salida del modelo\n",
        "                        output, val_h = model(inputs, val_h)\n",
        "                        \n",
        "                        # calcular la pérdida\n",
        "                        val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                    \n",
        "                        # almacenar las pérdidas\n",
        "                        val_losses.append(val_loss.item())\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "M8uGZdlw85eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota**: Se deben de crear nuevas variables para el estado oculto `h`, de lo contrario, se retrodecería a través de todo el historial de entrenamiento.\n",
        "\n",
        "El problema se encuentra en el ciclo de entrenamiento: `backward()`  está tratando de propagarse hacia atrás hasta el comienzo del tiempo, lo que funciona para el primer lote pero no para el segundo porque el grafo del primer lote se ha descartado. **esto genera un error**.<br>\n",
        "<br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LxGkmTwpkg3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Realizar entrenamiento"
      ],
      "metadata": {
        "id": "rS5cjDkm9hyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "model = LSTMtextGeneration(chars, n_hidden, n_layers)\n",
        "print(model)\n",
        "batch_size = 256\n",
        "seq_length = 10\n",
        "n_epochs =  10\n",
        "\n",
        "# entrenar el modelo\n",
        "train(model, encoded_text, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTa2QZUG9a2i",
        "outputId": "1355638a-aecc-4f8e-d980-4174905a39df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMtextGeneration(\n",
            "  (lstm): LSTM(95, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=95, bias=True)\n",
            ")\n",
            "Epoch: 1/10... Step: 10... Loss: 3.2262... Val Loss: 3.2620\n",
            "Epoch: 1/10... Step: 20... Loss: 3.1583... Val Loss: 3.1877\n",
            "Epoch: 1/10... Step: 30... Loss: 3.1047... Val Loss: 3.1656\n",
            "Epoch: 1/10... Step: 40... Loss: 3.0880... Val Loss: 3.1580\n",
            "Epoch: 2/10... Step: 50... Loss: 3.0999... Val Loss: 3.1523\n",
            "Epoch: 2/10... Step: 60... Loss: 3.0808... Val Loss: 3.1464\n",
            "Epoch: 2/10... Step: 70... Loss: 3.1188... Val Loss: 3.1405\n",
            "Epoch: 2/10... Step: 80... Loss: 3.0448... Val Loss: 3.1303\n",
            "Epoch: 3/10... Step: 90... Loss: 3.1424... Val Loss: 3.1058\n",
            "Epoch: 3/10... Step: 100... Loss: 3.0486... Val Loss: 3.0530\n",
            "Epoch: 3/10... Step: 110... Loss: 2.9201... Val Loss: 3.0018\n",
            "Epoch: 3/10... Step: 120... Loss: 2.8110... Val Loss: 2.9102\n",
            "Epoch: 3/10... Step: 130... Loss: 2.7563... Val Loss: 2.8687\n",
            "Epoch: 4/10... Step: 140... Loss: 2.6725... Val Loss: 2.7585\n",
            "Epoch: 4/10... Step: 150... Loss: 2.5988... Val Loss: 2.7060\n",
            "Epoch: 4/10... Step: 160... Loss: 2.5917... Val Loss: 2.6368\n",
            "Epoch: 4/10... Step: 170... Loss: 2.5068... Val Loss: 2.6015\n",
            "Epoch: 5/10... Step: 180... Loss: 2.4908... Val Loss: 2.5655\n",
            "Epoch: 5/10... Step: 190... Loss: 2.4485... Val Loss: 2.5318\n",
            "Epoch: 5/10... Step: 200... Loss: 2.4514... Val Loss: 2.5147\n",
            "Epoch: 5/10... Step: 210... Loss: 2.3786... Val Loss: 2.4843\n",
            "Epoch: 5/10... Step: 220... Loss: 2.5327... Val Loss: 2.4577\n",
            "Epoch: 6/10... Step: 230... Loss: 2.3662... Val Loss: 2.4306\n",
            "Epoch: 6/10... Step: 240... Loss: 2.3388... Val Loss: 2.4071\n",
            "Epoch: 6/10... Step: 250... Loss: 2.2999... Val Loss: 2.4119\n",
            "Epoch: 6/10... Step: 260... Loss: 2.3038... Val Loss: 2.3770\n",
            "Epoch: 7/10... Step: 270... Loss: 2.2267... Val Loss: 2.3671\n",
            "Epoch: 7/10... Step: 280... Loss: 2.2143... Val Loss: 2.3374\n",
            "Epoch: 7/10... Step: 290... Loss: 2.2611... Val Loss: 2.3306\n",
            "Epoch: 7/10... Step: 300... Loss: 2.1793... Val Loss: 2.3159\n",
            "Epoch: 8/10... Step: 310... Loss: 2.1943... Val Loss: 2.2940\n",
            "Epoch: 8/10... Step: 320... Loss: 2.1777... Val Loss: 2.2897\n",
            "Epoch: 8/10... Step: 330... Loss: 2.1288... Val Loss: 2.2793\n",
            "Epoch: 8/10... Step: 340... Loss: 2.1287... Val Loss: 2.2640\n",
            "Epoch: 8/10... Step: 350... Loss: 2.0983... Val Loss: 2.2443\n",
            "Epoch: 9/10... Step: 360... Loss: 2.1334... Val Loss: 2.2289\n",
            "Epoch: 9/10... Step: 370... Loss: 2.1273... Val Loss: 2.2223\n",
            "Epoch: 9/10... Step: 380... Loss: 2.1149... Val Loss: 2.2268\n",
            "Epoch: 9/10... Step: 390... Loss: 2.1024... Val Loss: 2.2088\n",
            "Epoch: 10/10... Step: 400... Loss: 2.1108... Val Loss: 2.2196\n",
            "Epoch: 10/10... Step: 410... Loss: 2.0846... Val Loss: 2.1884\n",
            "Epoch: 10/10... Step: 420... Loss: 2.0680... Val Loss: 2.1812\n",
            "Epoch: 10/10... Step: 430... Loss: 2.0448... Val Loss: 2.1750\n",
            "Epoch: 10/10... Step: 440... Loss: 2.3038... Val Loss: 2.1671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir función para predecir\n"
      ],
      "metadata": {
        "id": "f0zKhO5I9knE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, char, h=None, top_k=None):\n",
        "      \"\"\"\n",
        "      dado un carácter, predecir el siguiente carácter.\n",
        "      devuelve el carácter predicho y el estado oculto\n",
        "      \"\"\"\n",
        "      \n",
        "      # codificar el carácter a su índice\n",
        "      x = np.array([[model.char2int[char]]])\n",
        "\n",
        "      # usar one-hot encode para el carácter\n",
        "      x = oneHotEncode(x, len(model.chars))\n",
        "\n",
        "      # convertir los datos a tensores\n",
        "      inputs = torch.from_numpy(x)\n",
        "      \n",
        "      # si hay cuda disponible definir los datos así\n",
        "      if(train_on_gpu): inputs = inputs.cuda()\n",
        "      \n",
        "      # separar el estado oculto de los estados h. (detach)\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      # obtener la salida del modelo\n",
        "      out, h = model(inputs, h)\n",
        "\n",
        "      # obtener las probabilidades del carácter\n",
        "      p = F.softmax(out, dim=1).data\n",
        "\n",
        "      # calcular los datos en modo cpu\n",
        "      if(train_on_gpu): p = p.cpu()\n",
        "      \n",
        "      # obtener los mejores caracteres predichos\n",
        "      if top_k is None:\n",
        "          top_ch = np.arange(len(model.chars))\n",
        "      else:\n",
        "          p, top_ch = p.topk(top_k)\n",
        "          top_ch = top_ch.numpy().squeeze()\n",
        "      \n",
        "      # seleccionar el próximo carácter probable con algún elemento de aleatoriedad\n",
        "      p = p.numpy().squeeze()\n",
        "      char = np.random.choice(top_ch, p=p/p.sum())\n",
        "      \n",
        "      # devolver el valor codificado del carácter predicho y el estado oculto\n",
        "      return model.int2char[char], h"
      ],
      "metadata": {
        "id": "6nDDp9XL9qs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir funcion para generar texto"
      ],
      "metadata": {
        "id": "sr3yUR0h9sJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, size, prime='el', top_k=None):\n",
        "    \"\"\"\n",
        "    generar texto a partir de una semilla\n",
        "    \"\"\"\n",
        "\n",
        "    if(train_on_gpu): model.cuda()\n",
        "    else: model.cpu()\n",
        "    \n",
        "    model.eval() # evaluar el modelo\n",
        "    \n",
        "    # obtener los caracteres de la semilla\n",
        "    chars = [ch for ch in prime]\n",
        "\n",
        "    # inicializar el estado oculto\n",
        "    h = model.init_hidden(1)\n",
        "    \n",
        "    # predecir los siguientes caracteres\n",
        "    for ch in prime:\n",
        "        char, h = predict(model, ch, h, top_k=top_k)\n",
        "    \n",
        "    # almacenar caracteres predichos\n",
        "    chars.append(char)\n",
        "    \n",
        "    # pasar el carácter anterior y obtén uno nuevo\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "1WXRrN5l9xcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generar Texto"
      ],
      "metadata": {
        "id": "67q-m9StggE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, 200, prime='hoy', top_k=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci9TeH10glYd",
        "outputId": "e8f77733-eba4-4430-fecb-2d7654167422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hoy entran en el cuento en el prentera de la menteran de la conta en el pale cono el preciaran en las prentos y lo conos del pontera dela caros, en el prente en lo comoses, y los\n",
            "conos los companto en el \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, 200, prime='hoy', top_k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72d1Rd2hj-wf",
        "outputId": "7bc13308-10d6-4236-f828-a90e39fe77af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hoy y a se pora de curate de el\n",
            "como espera la compenteron en\n",
            "los comistan lo mino de los cuarto, de el minterones, estero el compante el mariante en el misición delos diente las parento de la malerante, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, 200, prime='a menudo', top_k=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWJkd1MekJb-",
        "outputId": "e4158eff-ec71-40cc-d87e-7e8c73841abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a menudo de lo contaran en la minto en las cara en la caro de estre en la minos del como del camo pare de el como de esteronto,\n",
            "el el compara las pare lo compre en en las caro el como en la misira del cono en \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, 200, prime='noche', top_k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsT4tPn7kaG0",
        "outputId": "6421f21e-18b7-4a26-9e65-2a9207329672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noches a cuela en lo\n",
            "menco entras,\n",
            "pare el mistan en las cuarantes, es con el miesidar a por ala para la cuancon es corronen en\n",
            "pantentena en el canes cara el mesoro do las promesta a la puento, y al cuesta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio\n",
        "\n",
        "*   Utilizar un conjunto de datos diferente (libro en español) pude ser del proyecto Gutenberg\n",
        "*   Una opción puede ser fusionar el nuevo conjunto de datos con el utilizado en este notebook.\n"
      ],
      "metadata": {
        "id": "IW54WxyFd3Fu"
      }
    }
  ]
}